{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge craiglist data with synthetic census data\n",
    "\n",
    "Steps: \n",
    "\n",
    "1. Get craigslist data \n",
    " - Query from remote database in batches based on FIPS\n",
    " - Joing to census via FIPS code\n",
    "2. Get census data \n",
    " - Aggregate to BG\n",
    "3. Merge together\n",
    "\n",
    "Can process in batches, one state at a time, or get data by region.\n",
    "\n",
    "TODO: \n",
    " - The census data for DC are missing. The files are there but they contain no data.\n",
    " - If need to look at education or other person data, aggregate person data to household and then block group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import paramiko\n",
    "import os\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Craigslist data table columns\n",
    "\n",
    "pid          | character varying(25)  |\n",
    " date         | date                   | \n",
    " region       | character varying(50)  | \n",
    " neighborhood | character varying(200) | \n",
    " rent         | double precision       | \n",
    " bedrooms     | double precision       | \n",
    " sqft         | double precision       | \n",
    " rent_sqft    | double precision       | \n",
    " longitude    | double precision       | \n",
    " latitude     | double precision       | \n",
    " county       | character varying(20)  | \n",
    " fips_block   | character varying(20)  | \n",
    " state        | character varying(20)  | \n",
    " bathrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIPS code format**\n",
    "\n",
    " 53-----033---001701--1--015\n",
    "\n",
    "[state][county][tract][bg][block]\n",
    "\n",
    "Note: for DC, county='001'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic census data - variables\n",
    "\n",
    "**Household data**: household_id,serialno,persons,cars,income,race_of_head,age_of_head,workers,state,county,tract,block group,children,tenure,recent_mover\n",
    "\n",
    "**Person data**: person_id,member_id,age,relate,edu,sex,hours,earning,race_id,household_id,student,work_at_home,worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote connection parameters\n",
    "\n",
    "Make sure you have two files saved in the same directory, postgres_settings.json and ssh_settings.json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR=os.path.join('..','data') \n",
    "\"\"\"Path to local data directory\"\"\"\n",
    "\n",
    "#read postgres connection parameters\n",
    "with open('postgres_settings.json') as settings_file:    \n",
    "    settings = json.load(settings_file)\n",
    "\n",
    "DBNAME = settings['dbname']\n",
    "USER = settings['user']\n",
    "HOST = settings['host']\n",
    "PASSWORD = settings['password']\n",
    "\n",
    "conn_str = \"dbname = {0} user = {1} host = {2} password = {3}\".format(DBNAME, USER, HOST, PASSWORD)\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(conn_str)\n",
    "    cur = conn.cursor()\n",
    "except:\n",
    "    print (\"Cannot connection. Check settings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: add putty connection too. \n",
    "#read SSH connection parameters\n",
    "with open('ssh_settings.json') as settings_file:    \n",
    "    settings = json.load(settings_file)\n",
    "\n",
    "HOSTNAME = settings['hostname']\n",
    "USERNAME = settings['username']\n",
    "PASSWORD = settings['password']\n",
    "LOCAL_KEY_DIR = settings['local_key_dir']\n",
    "\n",
    "CENSUS_DIR = 'synthetic_population'\n",
    "\"\"\"Remote directory with census data\"\"\"\n",
    "\n",
    "RESULTS_DIR = 'craigslist_census'\n",
    "\"\"\"Remote directory for results\"\"\"\n",
    "\n",
    "# estbalish SSH connection\n",
    "ssh = paramiko.SSHClient() \n",
    "ssh.load_host_keys(LOCAL_KEY_DIR)\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "ssh.connect(HOSTNAME,username=USERNAME, password=PASSWORD)\n",
    "sftp = ssh.open_sftp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create FIPS look-up tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>MPO_ID</th>\n",
       "      <th>MPONAME</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STFIPS</th>\n",
       "      <th>STATEFP</th>\n",
       "      <th>COUNTYFP</th>\n",
       "      <th>COUNTYNS</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>...</th>\n",
       "      <th>CBSAFP</th>\n",
       "      <th>METDIVFP</th>\n",
       "      <th>FUNCSTAT</th>\n",
       "      <th>ALAND</th>\n",
       "      <th>AWATER</th>\n",
       "      <th>INTPTLAT</th>\n",
       "      <th>INTPTLON</th>\n",
       "      <th>PERIMETER</th>\n",
       "      <th>area_sqmi</th>\n",
       "      <th>st_co_fips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1826900.0</td>\n",
       "      <td>47198201</td>\n",
       "      <td>Johnson City Metropolitan Transportation Plann...</td>\n",
       "      <td>TN</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>163</td>\n",
       "      <td>1639793</td>\n",
       "      <td>47163</td>\n",
       "      <td>Sullivan</td>\n",
       "      <td>...</td>\n",
       "      <td>28700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>1.070725e+09</td>\n",
       "      <td>4.220920e+07</td>\n",
       "      <td>36.510213</td>\n",
       "      <td>-82.299396</td>\n",
       "      <td>71837.801334</td>\n",
       "      <td>0.756540</td>\n",
       "      <td>47163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1541707.0</td>\n",
       "      <td>29201300</td>\n",
       "      <td>Southeast Metropolitan Planning Organization (...</td>\n",
       "      <td>MO</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "      <td>003</td>\n",
       "      <td>424203</td>\n",
       "      <td>17003</td>\n",
       "      <td>Alexander</td>\n",
       "      <td>...</td>\n",
       "      <td>16020.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>6.099969e+08</td>\n",
       "      <td>4.423719e+07</td>\n",
       "      <td>37.183657</td>\n",
       "      <td>-89.349516</td>\n",
       "      <td>6972.897419</td>\n",
       "      <td>0.591829</td>\n",
       "      <td>29003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2101128.0</td>\n",
       "      <td>51197401</td>\n",
       "      <td>Richmond Area MPO</td>\n",
       "      <td>VA</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>670</td>\n",
       "      <td>1498428</td>\n",
       "      <td>51670</td>\n",
       "      <td>Hopewell</td>\n",
       "      <td>...</td>\n",
       "      <td>40060.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>2.665508e+07</td>\n",
       "      <td>1.324078e+06</td>\n",
       "      <td>37.291010</td>\n",
       "      <td>-77.298944</td>\n",
       "      <td>6041.591069</td>\n",
       "      <td>0.257294</td>\n",
       "      <td>51670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>220693.0</td>\n",
       "      <td>15201300</td>\n",
       "      <td>Maui MPO</td>\n",
       "      <td>HI</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>009</td>\n",
       "      <td>365283</td>\n",
       "      <td>15009</td>\n",
       "      <td>Maui</td>\n",
       "      <td>...</td>\n",
       "      <td>27980.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>-1.286608e+09</td>\n",
       "      <td>-1.091674e+09</td>\n",
       "      <td>20.855931</td>\n",
       "      <td>-156.601550</td>\n",
       "      <td>299148.486465</td>\n",
       "      <td>728.079214</td>\n",
       "      <td>15009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>220707.0</td>\n",
       "      <td>15197500</td>\n",
       "      <td>Oahu MPO</td>\n",
       "      <td>HI</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>003</td>\n",
       "      <td>365281</td>\n",
       "      <td>15003</td>\n",
       "      <td>Honolulu</td>\n",
       "      <td>...</td>\n",
       "      <td>46520.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>1.555505e+09</td>\n",
       "      <td>-2.400273e+08</td>\n",
       "      <td>21.461365</td>\n",
       "      <td>-158.201974</td>\n",
       "      <td>329335.691836</td>\n",
       "      <td>610.910815</td>\n",
       "      <td>15003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    MPO_ID                                            MPONAME  \\\n",
       "0  1826900.0  47198201  Johnson City Metropolitan Transportation Plann...   \n",
       "1  1541707.0  29201300  Southeast Metropolitan Planning Organization (...   \n",
       "2  2101128.0  51197401                                  Richmond Area MPO   \n",
       "3   220693.0  15201300                                           Maui MPO   \n",
       "4   220707.0  15197500                                           Oahu MPO   \n",
       "\n",
       "  STATE STFIPS  STATEFP COUNTYFP  COUNTYNS  GEOID       NAME     ...      \\\n",
       "0    TN     47       47      163   1639793  47163   Sullivan     ...       \n",
       "1    MO     29       17      003    424203  17003  Alexander     ...       \n",
       "2    VA     51       51      670   1498428  51670   Hopewell     ...       \n",
       "3    HI     15       15      009    365283  15009       Maui     ...       \n",
       "4    HI     15       15      003    365281  15003   Honolulu     ...       \n",
       "\n",
       "    CBSAFP  METDIVFP FUNCSTAT         ALAND        AWATER   INTPTLAT  \\\n",
       "0  28700.0       NaN        A  1.070725e+09  4.220920e+07  36.510213   \n",
       "1  16020.0       NaN        A  6.099969e+08  4.423719e+07  37.183657   \n",
       "2  40060.0       NaN        F  2.665508e+07  1.324078e+06  37.291010   \n",
       "3  27980.0       NaN        A -1.286608e+09 -1.091674e+09  20.855931   \n",
       "4  46520.0       NaN        A  1.555505e+09 -2.400273e+08  21.461365   \n",
       "\n",
       "     INTPTLON      PERIMETER   area_sqmi  st_co_fips  \n",
       "0  -82.299396   71837.801334    0.756540       47163  \n",
       "1  -89.349516    6972.897419    0.591829       29003  \n",
       "2  -77.298944    6041.591069    0.257294       51670  \n",
       "3 -156.601550  299148.486465  728.079214       15009  \n",
       "4 -158.201974  329335.691836  610.910815       15003  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dictionary of states and fips codes. \n",
    "fips_state = pd.read_csv(os.path.join(DATA_DIR,'state_fips_codes.csv'),dtype=str)\n",
    "fips2state=dict(zip(fips_state['FIPS'],fips_state['USPS']))\n",
    "state2fips=dict(zip(fips_state['USPS'],fips_state['FIPS']))\n",
    "\n",
    "# Make lookup for county to MPO code \n",
    "mpo_counties = pd.read_csv(os.path.join(DATA_DIR,'us_2015_mpo_regions_counties_v1.csv'), encoding='latin1', dtype={'MPO_ID':str,'COUNTYFP':str,'STFIPS':str})\n",
    "mpo_counties['COUNTYFP'] = mpo_counties['COUNTYFP'].str.zfill(2)  \n",
    "mpo_counties['st_co_fips'] = mpo_counties['STFIPS']+mpo_counties['COUNTYFP']  # we will want to join on 2-char state + 3-char county fips\n",
    "county2mpo=dict(zip(mpo_counties['st_co_fips'],mpo_counties['MPO_ID']))  # do we want MPO_ID or do we want GEOID? \n",
    "mpo_counties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_query(q):\n",
    "    \"\"\" Get results given SQL query\"\"\"\n",
    "    cur.execute(q)\n",
    "    return(cur.fetchall())\n",
    "\n",
    "def get_craiglist(filters):\n",
    "    \"\"\"Get craiglist data from database.\n",
    "    Args: \n",
    "        filters (list): list of strings containing filter criteria. Format as individual SQL WHERE statements. E.g., [\"region='sandiego'\",\"rent>100\"]\n",
    "    Returns: \n",
    "        DataFrame: listings data. \n",
    "    \"\"\"\n",
    "    #q=\"SELECT pid,date,rent,bedrooms,bathrooms,sqft,rent_sqft,fips_block,state,region,longitude,latitude FROM rental_listings WHERE state='{}';\".format(state)\n",
    "    filters_str = ' AND '.join([x for x in filters])\n",
    "    q=\"SELECT pid,date,rent,bedrooms,bathrooms,sqft,rent_sqft,fips_block,state,region,longitude,latitude FROM rental_listings WHERE {};\".format(filters_str)\n",
    "    results=run_query(q)\n",
    "    df=pd.DataFrame(results,columns=['listing_id', 'date','rent','bedrooms','bathrooms','sqft','rent_sqft','fips_block','state','region','lng','lat'] )  # put it all into a dataframe\n",
    "    \n",
    "    # split FIPS into different columns - split off the last 3 chars\n",
    "    df['block']=df.fips_block.str[-4:]\n",
    "    df['fips12']=df.fips_block.str[:-3]\n",
    "    return(df)\n",
    "\n",
    "def read_census_file(fname):\n",
    "    \"\"\"Read census csv file via SFTP and return as dataframe.\"\"\"\n",
    "    with sftp.open(os.path.join(CENSUS_DIR,fname)) as f:\n",
    "        df = pd.read_csv(f, delimiter=',',dtype={'age_of_head':float, 'block group':str, 'cars':float, 'children':float, 'county':str,\n",
    "           'household_id':str, 'income':float, 'persons':float, 'race_of_head':str, 'recent_mover':str,\n",
    "           'serialno':str, 'state':str, 'tenure':str, 'tract':str, 'workers':float})\n",
    "    return df\n",
    "\n",
    "def write_results_file(data,fname):\n",
    "    \"\"\"Write merged data to csv file via SFTP.\"\"\"\n",
    "    with sftp.open(os.path.join(RESULTS_DIR,fname),'w') as f:\n",
    "        data.to_csv(f,index=True)\n",
    "    return\n",
    "\n",
    "def get_census_by_state(state, table='households'): \n",
    "    \"\"\"Return all census data for state given two-char abbreviation. Can be 'households' or 'persons' data. \"\"\" \n",
    "    filelist=sftp.listdir(CENSUS_DIR)\n",
    "    if table=='households':\n",
    "        files = [f for f in filelist if f[:5]=='hh_{}'.format(state)]\n",
    "    elif table=='persons':\n",
    "        files = [f for f in filelist if f[:4]=='p_{}'.format(state)]\n",
    "    #files = files[:3]  # uncomment this line for testing.\n",
    "    new_df = pd.DataFrame()\n",
    "    for f in files:\n",
    "        df = read_census_file(f)\n",
    "        new_df = pd.concat([new_df,df])\n",
    "    return(new_df)\n",
    "\n",
    "def strip_zeros(s):\n",
    "    \"\"\"Remove '.0 from end of string\"\"\"\n",
    "    if s.endswith('.0'):\n",
    "        return(s[:-2])\n",
    "    else:\n",
    "        return(s)\n",
    "\n",
    "def format_hh_data(df):\n",
    "    \"\"\"Fix formatting for hhs census data. Replace '' strings with zero. Format other strings.\"\"\"\n",
    "\n",
    "    df['county'] = df['county'].str.zfill(2)  # make county 3-char string.\n",
    "    \n",
    "    for col in ['children','workers']:\n",
    "        df[col] = df[col].replace('','0')\n",
    "\n",
    "    for col in ['race_of_head','recent_mover','tenure']:\n",
    "        df[col] = df[col].astype(str)\n",
    "        df[col] = df[col].map(strip_zeros)  # make sure strings are formatted. \n",
    "    return(df)\n",
    "\n",
    "def aggregate_census(df, groupby_cols=['county','tract','block group'],cols_to_sum=['cars','children','persons','workers'], cols_to_median=['age_of_head','income'],categ_cols=['race_of_head','recent_mover','tenure'],id_col='serialno',table='hhs'):\n",
    "    \"\"\"Aggregate census table to block group. Made this for hh data, may need to revised for persons data.\n",
    "    Args: \n",
    "        groupby_cols (list): names of columns to group by (default=['county','tract','block group'])\n",
    "        cols_to_sum (list): names of columns for which to compute totals. \n",
    "        cols_to_median (list): names of columns for which to compute medians\n",
    "        categ_cols (list): names of categorical columns\n",
    "        id_col (str): name of column that serves as the id column, to use in counting rows. \n",
    "        table (str): 'hhs' (default) or 'per'\n",
    "    Returns: \n",
    "        DataFrame: aggregated data. \n",
    "        \"\"\"\n",
    "    # For some columns we'll want to find the sum or average/median. These will need only a simple groupby \n",
    "    sums = df.groupby(by=groupby_cols).sum()[cols_to_sum]\n",
    "    sums.columns = [x+'_tot' for x in cols_to_sum]\n",
    "    \n",
    "    medians = df.groupby(by=groupby_cols).median()[cols_to_median]\n",
    "    medians.columns = [x+'_med' for x in cols_to_median]\n",
    "    \n",
    "    counts = pd.DataFrame(df.groupby(by=groupby_cols).count()[id_col])\n",
    "    counts.columns=[table+'_tot']\n",
    "\n",
    "    # Categorical columns will need pivot tables. \n",
    "    categoricals = pd.DataFrame(index=counts.index)\n",
    "    for col in categ_cols:\n",
    "        pivoted=df.pivot_table(index = groupby_cols, columns = col, aggfunc='count')[id_col]\n",
    "        pivoted.columns = [col+'_'+x for x in pivoted.columns]\n",
    "        pivoted.columns = pivoted.columns.map(strip_zeros)\n",
    "        # merge back together\n",
    "        categoricals = pd.merge(categoricals, pivoted, left_index=True, right_index=True)\n",
    "\n",
    "    # put all back together in one table\n",
    "    merged = pd.merge(sums, medians, left_index=True, right_index=True)\n",
    "    merged = pd.merge(merged, counts, left_index=True, right_index=True)\n",
    "    merged = pd.merge(merged, categoricals, left_index=True, right_index=True)\n",
    "\n",
    "    # check lengths of dataframes to detect any problems in grouping or merging\n",
    "    lengths = [len(sums),len(medians),len(counts),len(categoricals),len(merged)]\n",
    "    if len(set(lengths))>1:\n",
    "        print('Warning: Aggregated tables have different lengths.',lengths,'for sums, medians, counts, categoricals, and merged.')\n",
    "    \n",
    "    return(merged)\n",
    "\n",
    "def match_mpo(s, mpo_dict=county2mpo):\n",
    "    \"\"\"Match a 5-char state-county FIPS code to an MPO code\n",
    "    Args: \n",
    "        s (str): 5-char state-county string\n",
    "        mpo_dict (dict): county2mpo dictionary\n",
    "    Returns: \n",
    "        str: MPO code\n",
    "    \"\"\"\n",
    "    try: \n",
    "        return mpo_dict[s]\n",
    "    except KeyError: # in this case, the county is not in an MPO\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_all(state, filters=None):\n",
    "    \"\"\"Get craigslist data and merge with census data, by state, and save.  with additional filters if needed. \n",
    "    Args: \n",
    "        state (str): 2-char state abbreviation\n",
    "        filters (list): additional filters. Do not need to include state in filter list\n",
    "    \"\"\"\n",
    "    \n",
    "    # load and prepare craiglist data\n",
    "    \n",
    "    # If filters are provided, use them to filter data\n",
    "    if filters:\n",
    "        filters.append(\"state='{}'\".format(state))\n",
    "        print(filters)\n",
    "        df_cl=get_craiglist(filters)\n",
    "    # If no filters provided, get all data for the specified state. \n",
    "    else:\n",
    "        df_cl=get_craiglist([\"state='{}'\".format(state)])\n",
    "    \n",
    "    df_cl['st_co_fps'] = df_cl.fips_block.map(lambda x: x[:5])\n",
    "    df_cl['mpo_id'] = df_cl.st_co_fps.map(match_mpo)\n",
    "\n",
    "    # load and prepare census data for households\n",
    "    hhs = get_census_by_state(state, table='households')\n",
    "    hhs = format_hh_data(hhs)\n",
    "    hhs_bg = aggregate_census(hhs)\n",
    "    hhs_bg=hhs_bg.reset_index()\n",
    "    hhs_bg['fips12']=state2fips[state]+hhs_bg['county']+hhs_bg['tract']+hhs_bg['block group'] # create 12-digit FIPS code for merging. \n",
    "\n",
    "    # merge with craigslist data. \n",
    "    merged = pd.merge(df_cl, hhs_bg, on='fips12',how='left')\n",
    "    merged = merged.set_index('listing_id')\n",
    "\n",
    "    #TODO: add persons data here, if needed. \n",
    "\n",
    "    # Keep only columns we'll need.\n",
    "    cols_to_keep=['date','rent','bedrooms','bathrooms','sqft','rent_sqft','fips_block','state','region','mpo_id','lng','lat','cars_tot','children_tot','persons_tot','workers_tot','age_of_head_med','income_med','hhs_tot','race_of_head_1','race_of_head_2','race_of_head_3','race_of_head_4','race_of_head_5','race_of_head_6','race_of_head_7','race_of_head_8','race_of_head_9','recent_mover_0','recent_mover_1','tenure_1','tenure_2']\n",
    "    \n",
    "    # This is a bit of a hack in case some columns are missing in some states. \n",
    "    for col in cols_to_keep: \n",
    "        if col not in merged.columns:\n",
    "            merged[col] = np.nan\n",
    "\n",
    "    # save file either locally or remotely. \n",
    "    print('Saving data for {s}: {m} rows'.format(s=state,m=len(merged)))\n",
    "    outfile = 'cl_census_{}.csv'.format(state)\n",
    "    #merged[cols_to_keep].to_csv(os.path.join(DATA_DIR,outfile), index=True)  # uncomment to save locally\n",
    "\n",
    "    #write_results_file(merged[cols_to_keep], outfile)  # uncomment to save remotely. \n",
    "    return merged[cols_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data for a single region\n",
    "\n",
    "Use this to get data for a single region, for use in our preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"region = 'sfbay'\", 'rent>0', \"state='CA'\"]\n",
      "Saving data for CA: 125539 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>rent</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft</th>\n",
       "      <th>rent_sqft</th>\n",
       "      <th>fips_block</th>\n",
       "      <th>state</th>\n",
       "      <th>region</th>\n",
       "      <th>mpo_id</th>\n",
       "      <th>...</th>\n",
       "      <th>race_of_head_4</th>\n",
       "      <th>race_of_head_5</th>\n",
       "      <th>race_of_head_6</th>\n",
       "      <th>race_of_head_7</th>\n",
       "      <th>race_of_head_8</th>\n",
       "      <th>race_of_head_9</th>\n",
       "      <th>recent_mover_0</th>\n",
       "      <th>recent_mover_1</th>\n",
       "      <th>tenure_1</th>\n",
       "      <th>tenure_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>listing_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5925013098</th>\n",
       "      <td>2016-12-18</td>\n",
       "      <td>2495.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>398.0</td>\n",
       "      <td>6.268844</td>\n",
       "      <td>060750162001001</td>\n",
       "      <td>CA</td>\n",
       "      <td>sfbay</td>\n",
       "      <td>06197001</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>378.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5925007228</th>\n",
       "      <td>2016-12-18</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1357.0</td>\n",
       "      <td>2.431835</td>\n",
       "      <td>060133342005000</td>\n",
       "      <td>CA</td>\n",
       "      <td>sfbay</td>\n",
       "      <td>06197001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>357.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5925014745</th>\n",
       "      <td>2016-12-18</td>\n",
       "      <td>1657.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>915.0</td>\n",
       "      <td>1.810929</td>\n",
       "      <td>060855091022006</td>\n",
       "      <td>CA</td>\n",
       "      <td>sfbay</td>\n",
       "      <td>06197001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>782.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>707.0</td>\n",
       "      <td>539.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1074.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5921274233</th>\n",
       "      <td>2016-12-18</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>060871006002003</td>\n",
       "      <td>CA</td>\n",
       "      <td>sfbay</td>\n",
       "      <td>06197502</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>337.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5925012514</th>\n",
       "      <td>2016-12-18</td>\n",
       "      <td>2195.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>443.0</td>\n",
       "      <td>4.954853</td>\n",
       "      <td>060750125011002</td>\n",
       "      <td>CA</td>\n",
       "      <td>sfbay</td>\n",
       "      <td>06197001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1190.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date    rent  bedrooms  bathrooms    sqft  rent_sqft  \\\n",
       "listing_id                                                               \n",
       "5925013098  2016-12-18  2495.0       0.0        NaN   398.0   6.268844   \n",
       "5925007228  2016-12-18  3300.0       2.0        NaN  1357.0   2.431835   \n",
       "5925014745  2016-12-18  1657.0       2.0        NaN   915.0   1.810929   \n",
       "5921274233  2016-12-18  1700.0       1.0        NaN   500.0   3.400000   \n",
       "5925012514  2016-12-18  2195.0       1.0        NaN   443.0   4.954853   \n",
       "\n",
       "                 fips_block state region    mpo_id    ...     race_of_head_4  \\\n",
       "listing_id                                            ...                      \n",
       "5925013098  060750162001001    CA  sfbay  06197001    ...                1.0   \n",
       "5925007228  060133342005000    CA  sfbay  06197001    ...                NaN   \n",
       "5925014745  060855091022006    CA  sfbay  06197001    ...                NaN   \n",
       "5921274233  060871006002003    CA  sfbay  06197502    ...                NaN   \n",
       "5925012514  060750125011002    CA  sfbay  06197001    ...                NaN   \n",
       "\n",
       "            race_of_head_5  race_of_head_6  race_of_head_7  race_of_head_8  \\\n",
       "listing_id                                                                   \n",
       "5925013098             NaN            42.0             2.0            14.0   \n",
       "5925007228             NaN             NaN             NaN             NaN   \n",
       "5925014745             2.0           782.0             NaN             8.0   \n",
       "5921274233             NaN            14.0             NaN             NaN   \n",
       "5925012514            10.0           473.0             8.0            69.0   \n",
       "\n",
       "            race_of_head_9  recent_mover_0  recent_mover_1  tenure_1  tenure_2  \n",
       "listing_id                                                                      \n",
       "5925013098            27.0           276.0           153.0      51.0     378.0  \n",
       "5925007228             NaN           357.0            72.0     225.0     204.0  \n",
       "5925014745            24.0           707.0           539.0     172.0    1074.0  \n",
       "5921274233             NaN           337.0            68.0     268.0     137.0  \n",
       "5925012514            61.0          1124.0            68.0       2.0    1190.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bayarea = run_all(state='CA',filters=[\"region = 'sfbay'\",\"rent>0\"])   # define whatever filters you want here.\n",
    "df_bayarea.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save locally\n",
    "outfile = 'sfbay_listings_04092017.csv'\n",
    "df_bayarea.to_csv(os.path.join(DATA_DIR,outfile), index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all data by state\n",
    "Use this to merge all the data by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for state in fips_state['USPS']:# uncomment when done with testing. \n",
    "    if state != 'DC':   # the DC census data is missing. \n",
    "        print('\\n Working on',state)\n",
    "        df_state = run_all(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
