{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge craiglist data with synthetic census data\n",
    "\n",
    "Process in batches, one state at a time. Or get data by region.\n",
    "\n",
    "\n",
    "Steps: \n",
    "\n",
    "1. Get craigslist data \n",
    " - Query in batches based on FIPS\n",
    " - Joing to census via FIPS code\n",
    "2. Get census data \n",
    " - Aggregate to BG\n",
    "3. Merge together\n",
    "\n",
    "\n",
    "TODO: \n",
    " - The census data for DC are missing. The files are there but they contain no data.\n",
    " - Put connection passwords in a separate txt file.\n",
    " - If need to look at education or other person data, aggregate person data to household and then block group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import paramiko\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Craigslist data table columns\n",
    "\n",
    "pid          | character varying(25)  |\n",
    " date         | date                   | \n",
    " region       | character varying(50)  | \n",
    " neighborhood | character varying(200) | \n",
    " rent         | double precision       | \n",
    " bedrooms     | double precision       | \n",
    " sqft         | double precision       | \n",
    " rent_sqft    | double precision       | \n",
    " longitude    | double precision       | \n",
    " latitude     | double precision       | \n",
    " county       | character varying(20)  | \n",
    " fips_block   | character varying(20)  | \n",
    " state        | character varying(20)  | \n",
    " bathrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIPS code format**\n",
    "\n",
    " 53-----033---001701--1--015\n",
    "\n",
    "[state][county][tract][bg][block]\n",
    "\n",
    "Note: for DC, county='001'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic census data - variables\n",
    "\n",
    "**Household data**: household_id,serialno,persons,cars,income,race_of_head,age_of_head,workers,state,county,tract,block group,children,tenure,recent_mover\n",
    "\n",
    "**Person data**: person_id,member_id,age,relate,edu,sex,hours,earning,race_id,household_id,student,work_at_home,worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote connection parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir='../data/' \n",
    "\"\"\"Path to local data directory\"\"\"\n",
    "\n",
    "username='cy290e'\n",
    "hostname='169.229.154.119'\n",
    "db_name='craigslist'\n",
    "password='' #password to database.  IMPORTANT: do not save passwords in the notebook\n",
    "\"\"\"Postgres connection parameters\"\"\"\n",
    "\n",
    "# establish postgres connection\n",
    "conn = psycopg2.connect(\"dbname={d} user={u} host={h} password={pw}\".format(d=db_name, u=username, h=hostname, pw=password))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hostname = '169.229.154.119'\n",
    "username = 'cy290e'\n",
    "password  = ''  # password for key. IMPORTANT: do not save passwords in the notebook\n",
    "local_key_dir = '~/.ssh/known_hosts' # local dir with known hosts\n",
    "\"\"\"SSH connection parameters\"\"\"\n",
    "\n",
    "census_dir = 'synthetic_population/'\n",
    "\"\"\"Remote directory with census data\"\"\"\n",
    "\n",
    "results_dir = 'craigslist_census/'\n",
    "\"\"\"Remote directory for results\"\"\"\n",
    "\n",
    "# estbalish SSH connection\n",
    "ssh = paramiko.SSHClient() \n",
    "ssh.load_host_keys(local_key_dir)\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "ssh.connect(hostname,username=username, password=password)\n",
    "sftp = ssh.open_sftp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create FIPS look-up tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make dictionary of states and fips codes. \n",
    "fips_state = pd.read_csv(data_dir+'state_fips_codes.csv',dtype=str)\n",
    "fips2state=dict(zip(fips_state['FIPS'],fips_state['USPS']))\n",
    "state2fips=dict(zip(fips_state['USPS'],fips_state['FIPS']))\n",
    "\n",
    "# Make lookup for county to MPO code \n",
    "mpo_counties = pd.read_csv(data_dir+'us_2015_mpo_regions_counties_v1.csv', encoding='latin1', dtype={'MPO_ID':str,'COUNTYFP':str,'STFIPS':str})\n",
    "mpo_counties['COUNTYFP'] = mpo_counties['COUNTYFP'].str.zfill(2)  \n",
    "mpo_counties['st_co_fips'] = mpo_counties['STFIPS']+mpo_counties['COUNTYFP']  # we will want to join on 2-char state + 3-char county fips\n",
    "county2mpo=dict(zip(mpo_counties['st_co_fips'],mpo_counties['MPO_ID']))  # do we want MPO_ID or do we want GEOID? \n",
    "mpo_counties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_query(q):\n",
    "    \"\"\" Get results given SQL query\"\"\"\n",
    "    cur.execute(q)\n",
    "    return(cur.fetchall())\n",
    "\n",
    "def get_craiglist(filters):\n",
    "    \"\"\"Get craiglist data from database.\n",
    "    Args: \n",
    "        filters (list): list of strings containing filter criteria. Format as individual SQL WHERE statements. E.g., [\"region='sandiego'\",\"rent>100\"]\n",
    "    Returns: \n",
    "        DataFrame: listings data. \n",
    "    \"\"\"\n",
    "    #q=\"SELECT pid,date,rent,bedrooms,bathrooms,sqft,rent_sqft,fips_block,state,region,longitude,latitude FROM rental_listings WHERE state='{}';\".format(state)\n",
    "    filters_str = ' AND '.join([x for x in filters])\n",
    "    q=\"SELECT pid,date,rent,bedrooms,bathrooms,sqft,rent_sqft,fips_block,state,region,longitude,latitude FROM rental_listings WHERE {};\".format(filters_str)\n",
    "    results=run_query(q)\n",
    "    df=pd.DataFrame(results,columns=['listing_id', 'date','rent','bedrooms','bathrooms','sqft','rent_sqft','fips_block','state','region','lng','lat'] )  # put it all into a dataframe\n",
    "    \n",
    "    # split FIPS into different columns - split off the last 3 chars\n",
    "    df['block']=df.fips_block.str[-4:]\n",
    "    df['fips12']=df.fips_block.str[:-3]\n",
    "    return(df)\n",
    "\n",
    "def read_census_file(fname):\n",
    "    \"\"\"Read census csv file via SFTP and return as dataframe.\"\"\"\n",
    "    with sftp.open(census_dir+fname) as f:\n",
    "        df = pd.read_csv(f, delimiter=',',dtype={'age_of_head':float, 'block group':str, 'cars':float, 'children':float, 'county':str,\n",
    "           'household_id':str, 'income':float, 'persons':float, 'race_of_head':str, 'recent_mover':str,\n",
    "           'serialno':str, 'state':str, 'tenure':str, 'tract':str, 'workers':float})\n",
    "    return df\n",
    "\n",
    "def write_results_file(data,fname):\n",
    "    \"\"\"Write merged data to csv file via SFTP.\"\"\"\n",
    "    with sftp.open(results_dir+fname,'w') as f:\n",
    "        data.to_csv(f,index=True)\n",
    "    return\n",
    "\n",
    "def get_census_by_state(state, table='households'): \n",
    "    \"\"\"Return all census data for state given two-char abbreviation. Can be 'households' or 'persons' data. \"\"\" \n",
    "    filelist=sftp.listdir(census_dir)\n",
    "    if table=='households':\n",
    "        files = [f for f in filelist if f[:5]=='hh_{}'.format(state)]\n",
    "    elif table=='persons':\n",
    "        files = [f for f in filelist if f[:4]=='p_{}'.format(state)]\n",
    "    #files = files[:3]  # uncomment this line for testing.\n",
    "    new_df = pd.DataFrame()\n",
    "    for f in files:\n",
    "        df = read_census_file(f)\n",
    "        new_df = pd.concat([new_df,df])\n",
    "    return(new_df)\n",
    "\n",
    "def strip_zeros(s):\n",
    "    \"\"\"Remove '.0 from end of string\"\"\"\n",
    "    if s.endswith('.0'):\n",
    "        return(s[:-2])\n",
    "    else:\n",
    "        return(s)\n",
    "\n",
    "def format_hh_data(df):\n",
    "    \"\"\"Fix formatting for hhs census data. Replace '' strings with zero. Format other strings.\"\"\"\n",
    "\n",
    "    df['county'] = df['county'].str.zfill(2)  # make county 3-char string.\n",
    "    \n",
    "    for col in ['children','workers']:\n",
    "        df[col] = df[col].replace('','0')\n",
    "\n",
    "    for col in ['race_of_head','recent_mover','tenure']:\n",
    "        df[col] = df[col].astype(str)\n",
    "        df[col] = df[col].map(strip_zeros)  # make sure strings are formatted. \n",
    "    return(df)\n",
    "\n",
    "def aggregate_census(df, groupby_cols=['county','tract','block group'],cols_to_sum=['cars','children','persons','workers'], cols_to_median=['age_of_head','income'],categ_cols=['race_of_head','recent_mover','tenure'],id_col='serialno',table='hhs'):\n",
    "    \"\"\"Aggregate census table to block group. Made this for hh data, may need to revised for persons data.\n",
    "    Args: \n",
    "        groupby_cols (list): names of columns to group by (default=['county','tract','block group'])\n",
    "        cols_to_sum (list): names of columns for which to compute totals. \n",
    "        cols_to_median (list): names of columns for which to compute medians\n",
    "        categ_cols (list): names of categorical columns\n",
    "        id_col (str): name of column that serves as the id column, to use in counting rows. \n",
    "        table (str): 'hhs' (default) or 'per'\n",
    "    Returns: \n",
    "        DataFrame: aggregated data. \n",
    "        \"\"\"\n",
    "    # For some columns we'll want to find the sum or average/median. These will need only a simple groupby \n",
    "    sums = df.groupby(by=groupby_cols).sum()[cols_to_sum]\n",
    "    sums.columns = [x+'_tot' for x in cols_to_sum]\n",
    "    \n",
    "    medians = df.groupby(by=groupby_cols).median()[cols_to_median]\n",
    "    medians.columns = [x+'_med' for x in cols_to_median]\n",
    "    \n",
    "    counts = pd.DataFrame(df.groupby(by=groupby_cols).count()[id_col])\n",
    "    counts.columns=[table+'_tot']\n",
    "\n",
    "    # Categorical columns will need pivot tables. \n",
    "    categoricals = pd.DataFrame(index=counts.index)\n",
    "    for col in categ_cols:\n",
    "        pivoted=df.pivot_table(index = groupby_cols, columns = col, aggfunc='count')[id_col]\n",
    "        pivoted.columns = [col+'_'+x for x in pivoted.columns]\n",
    "        pivoted.columns = pivoted.columns.map(strip_zeros)\n",
    "        # merge back together\n",
    "        categoricals = pd.merge(categoricals, pivoted, left_index=True, right_index=True)\n",
    "\n",
    "    # put all back together in one table\n",
    "    merged = pd.merge(sums, medians, left_index=True, right_index=True)\n",
    "    merged = pd.merge(merged, counts, left_index=True, right_index=True)\n",
    "    merged = pd.merge(merged, categoricals, left_index=True, right_index=True)\n",
    "\n",
    "    # check lengths of dataframes to detect any problems in grouping or merging\n",
    "    lengths = [len(sums),len(medians),len(counts),len(categoricals),len(merged)]\n",
    "    if len(set(lengths))>1:\n",
    "        print('Warning: Aggregated tables have different lengths.',lengths,'for sums, medians, counts, categoricals, and merged.')\n",
    "    \n",
    "    return(merged)\n",
    "\n",
    "def match_mpo(s, mpo_dict=county2mpo):\n",
    "    \"\"\"Match a 5-char state-county FIPS code to an MPO code\n",
    "    Args: \n",
    "        s (str): 5-char state-county string\n",
    "        mpo_dict (dict): county2mpo dictionary\n",
    "    Returns: \n",
    "        str: MPO code\n",
    "    \"\"\"\n",
    "    try: \n",
    "        return mpo_dict[s]\n",
    "    except KeyError: # in this case, the county is not in an MPO\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_all(state, filters=None):\n",
    "    \"\"\"Get craigslist data and merge with census data, by state, and save.  with additional filters if needed. \n",
    "    Args: \n",
    "        state (str): 2-char state abbreviation\n",
    "        filters (list): additional filters. Do not need to include state in filter list\n",
    "    \"\"\"\n",
    "    \n",
    "    # load and prepare craiglist data\n",
    "    \n",
    "    # If filters are provided, use them to filter data\n",
    "    if filters:\n",
    "        filters.append(\"state='{}'\".format(state))\n",
    "        print(filters)\n",
    "        df_cl=get_craiglist(filters)\n",
    "    # If no filters provided, get all data for the specified state. \n",
    "    else:\n",
    "        df_cl=get_craiglist([\"state='{}'\".format(state)])\n",
    "    \n",
    "    df_cl['st_co_fps'] = df_cl.fips_block.map(lambda x: x[:5])\n",
    "    df_cl['mpo_id'] = df_cl.st_co_fps.map(match_mpo)\n",
    "\n",
    "    # load and prepare census data for households\n",
    "    hhs = get_census_by_state(state, table='households')\n",
    "    hhs = format_hh_data(hhs)\n",
    "    hhs_bg = aggregate_census(hhs)\n",
    "    hhs_bg=hhs_bg.reset_index()\n",
    "    hhs_bg['fips12']=state2fips[state]+hhs_bg['county']+hhs_bg['tract']+hhs_bg['block group'] # create 12-digit FIPS code for merging. \n",
    "\n",
    "    # merge with craigslist data. \n",
    "    merged = pd.merge(df_cl, hhs_bg, on='fips12',how='left')\n",
    "    merged = merged.set_index('listing_id')\n",
    "\n",
    "    #TODO: add persons data here, if needed. \n",
    "\n",
    "    # Keep only columns we'll need.\n",
    "    cols_to_keep=['date','rent','bedrooms','bathrooms','sqft','rent_sqft','fips_block','state','region','mpo_id','lng','lat','cars_tot','children_tot','persons_tot','workers_tot','age_of_head_med','income_med','hhs_tot','race_of_head_1','race_of_head_2','race_of_head_3','race_of_head_4','race_of_head_5','race_of_head_6','race_of_head_7','race_of_head_8','race_of_head_9','recent_mover_0','recent_mover_1','tenure_1','tenure_2']\n",
    "    \n",
    "    # This is a bit of a hack in case some columns are missing in some states. \n",
    "    for col in cols_to_keep: \n",
    "        if col not in merged.columns:\n",
    "            merged[col] = np.nan\n",
    "\n",
    "    # save file either locally or remotely. \n",
    "    print('Saving data for {s}: {m} rows'.format(s=state,m=len(merged)))\n",
    "    outfile = 'cl_census_{}.csv'.format(state)\n",
    "    #merged[cols_to_keep].to_csv(data_dir+outfile, index=True)  # uncomment to save locally\n",
    "\n",
    "    #write_results_file(merged[cols_to_keep], outfile)  # uncomment to save remotely. \n",
    "    return merged[cols_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data for a single region\n",
    "\n",
    "Use this to get data for a single region, for use in our preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_bayarea = run_all(state='CA',filters=[\"region = 'sfbay'\",\"rent>0\"])   # define whatever filters you want here.\n",
    "df_bayarea.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save locally\n",
    "outfile = 'sfbay_listings_03032017.csv'\n",
    "df_bayarea.to_csv(data_dir+outfile, index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all data by state\n",
    "Use this to merge all the data by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for state in fips_state['USPS']:# uncomment when done with testing. \n",
    "    if state != 'DC':   # the DC census data is missing. \n",
    "        print('\\n Working on',state)\n",
    "        df_state = run_all(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
