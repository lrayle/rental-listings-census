{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge craiglist data with synthetic census data\n",
    "\n",
    "Steps: \n",
    "\n",
    "1. Get craigslist data \n",
    " - Query from remote database in batches based on FIPS\n",
    " - Joing to census via FIPS code\n",
    "2. Get census data \n",
    " - Aggregate to BG\n",
    "3. Merge together\n",
    "\n",
    "Can process in batches, one state at a time, or get data by region.\n",
    "\n",
    "TODO: \n",
    " - The census data for DC are missing. The files are there but they contain no data.\n",
    " - If need to look at education or other person data, aggregate person data to household and then block group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import paramiko\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Craigslist data table columns\n",
    "\n",
    "pid          | character varying(25)  |\n",
    " date         | date                   | \n",
    " region       | character varying(50)  | \n",
    " neighborhood | character varying(200) | \n",
    " rent         | double precision       | \n",
    " bedrooms     | double precision       | \n",
    " sqft         | double precision       | \n",
    " rent_sqft    | double precision       | \n",
    " longitude    | double precision       | \n",
    " latitude     | double precision       | \n",
    " county       | character varying(20)  | \n",
    " fips_block   | character varying(20)  | \n",
    " state        | character varying(20)  | \n",
    " bathrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIPS code format**\n",
    "\n",
    " 53-----033---001701--1--015\n",
    "\n",
    "[state][county][tract][bg][block]\n",
    "\n",
    "Note: for DC, county='001'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic census data - variables\n",
    "\n",
    "**Household data**: household_id,serialno,persons,cars,income,race_of_head,age_of_head,workers,state,county,tract,block group,children,tenure,recent_mover\n",
    "\n",
    "**Person data**: person_id,member_id,age,relate,edu,sex,hours,earning,race_id,household_id,student,work_at_home,worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure connection parameters\n",
    "\n",
    "Make sure you have two files saved in the same directory, postgres_settings.json and ssh_settings.json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR=os.path.join('..','data') \n",
    "\"\"\"Path to local data directory\"\"\"\n",
    "\n",
    "#read postgres connection parameters\n",
    "with open('postgres_settings.json') as settings_file:    \n",
    "    settings = json.load(settings_file)\n",
    "\n",
    "DBNAME = settings['dbname']\n",
    "USER = settings['user']\n",
    "HOST = settings['host']\n",
    "PASSWORD = settings['password']\n",
    "\n",
    "conn_str = \"dbname = {0} user = {1} host = {2} password = {3}\".format(DBNAME, USER, HOST, PASSWORD)\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(conn_str)\n",
    "    cur = conn.cursor()\n",
    "except:\n",
    "    print (\"Cannot connection. Check settings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: add putty connection too. \n",
    "#read SSH connection parameters\n",
    "with open('ssh_settings.json') as settings_file:    \n",
    "    settings = json.load(settings_file)\n",
    "\n",
    "HOSTNAME = settings['hostname']\n",
    "USERNAME = settings['username']\n",
    "PASSWORD = settings['password']\n",
    "LOCAL_KEY_DIR = settings['local_key_dir']\n",
    "\n",
    "CENSUS_DIR = 'synthetic_population'\n",
    "\"\"\"Remote directory with census data\"\"\"\n",
    "\n",
    "RESULTS_DIR = 'craigslist_census'\n",
    "\"\"\"Remote directory for results\"\"\"\n",
    "\n",
    "# estbalish SSH connection\n",
    "ssh = paramiko.SSHClient() \n",
    "ssh.load_host_keys(LOCAL_KEY_DIR)\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "ssh.connect(HOSTNAME,username=USERNAME, password=PASSWORD)\n",
    "sftp = ssh.open_sftp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local data files\n",
    "\n",
    "Local files with with block-level data from urbansim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BLOCK_DIR = os.path.join('..','data','urbansim')\n",
    "BLOCK_ZFILE = 'ba_block_variables.csv.zip'\n",
    "BLOCK_FILE = 'ba_block_variables.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create FIPS look-up tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>MPO_ID</th>\n",
       "      <th>MPONAME</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STFIPS</th>\n",
       "      <th>STATEFP</th>\n",
       "      <th>COUNTYFP</th>\n",
       "      <th>COUNTYNS</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>...</th>\n",
       "      <th>CBSAFP</th>\n",
       "      <th>METDIVFP</th>\n",
       "      <th>FUNCSTAT</th>\n",
       "      <th>ALAND</th>\n",
       "      <th>AWATER</th>\n",
       "      <th>INTPTLAT</th>\n",
       "      <th>INTPTLON</th>\n",
       "      <th>PERIMETER</th>\n",
       "      <th>area_sqmi</th>\n",
       "      <th>st_co_fips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1826900.0</td>\n",
       "      <td>47198201</td>\n",
       "      <td>Johnson City Metropolitan Transportation Plann...</td>\n",
       "      <td>TN</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>163</td>\n",
       "      <td>1639793</td>\n",
       "      <td>47163</td>\n",
       "      <td>Sullivan</td>\n",
       "      <td>...</td>\n",
       "      <td>28700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>1.070725e+09</td>\n",
       "      <td>4.220920e+07</td>\n",
       "      <td>36.510213</td>\n",
       "      <td>-82.299396</td>\n",
       "      <td>71837.801334</td>\n",
       "      <td>0.756540</td>\n",
       "      <td>47163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1541707.0</td>\n",
       "      <td>29201300</td>\n",
       "      <td>Southeast Metropolitan Planning Organization (...</td>\n",
       "      <td>MO</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "      <td>003</td>\n",
       "      <td>424203</td>\n",
       "      <td>17003</td>\n",
       "      <td>Alexander</td>\n",
       "      <td>...</td>\n",
       "      <td>16020.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>6.099969e+08</td>\n",
       "      <td>4.423719e+07</td>\n",
       "      <td>37.183657</td>\n",
       "      <td>-89.349516</td>\n",
       "      <td>6972.897419</td>\n",
       "      <td>0.591829</td>\n",
       "      <td>29003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2101128.0</td>\n",
       "      <td>51197401</td>\n",
       "      <td>Richmond Area MPO</td>\n",
       "      <td>VA</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>670</td>\n",
       "      <td>1498428</td>\n",
       "      <td>51670</td>\n",
       "      <td>Hopewell</td>\n",
       "      <td>...</td>\n",
       "      <td>40060.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>2.665508e+07</td>\n",
       "      <td>1.324078e+06</td>\n",
       "      <td>37.291010</td>\n",
       "      <td>-77.298944</td>\n",
       "      <td>6041.591069</td>\n",
       "      <td>0.257294</td>\n",
       "      <td>51670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>220693.0</td>\n",
       "      <td>15201300</td>\n",
       "      <td>Maui MPO</td>\n",
       "      <td>HI</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>009</td>\n",
       "      <td>365283</td>\n",
       "      <td>15009</td>\n",
       "      <td>Maui</td>\n",
       "      <td>...</td>\n",
       "      <td>27980.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>-1.286608e+09</td>\n",
       "      <td>-1.091674e+09</td>\n",
       "      <td>20.855931</td>\n",
       "      <td>-156.601550</td>\n",
       "      <td>299148.486465</td>\n",
       "      <td>728.079214</td>\n",
       "      <td>15009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>220707.0</td>\n",
       "      <td>15197500</td>\n",
       "      <td>Oahu MPO</td>\n",
       "      <td>HI</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>003</td>\n",
       "      <td>365281</td>\n",
       "      <td>15003</td>\n",
       "      <td>Honolulu</td>\n",
       "      <td>...</td>\n",
       "      <td>46520.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>1.555505e+09</td>\n",
       "      <td>-2.400273e+08</td>\n",
       "      <td>21.461365</td>\n",
       "      <td>-158.201974</td>\n",
       "      <td>329335.691836</td>\n",
       "      <td>610.910815</td>\n",
       "      <td>15003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    MPO_ID                                            MPONAME  \\\n",
       "0  1826900.0  47198201  Johnson City Metropolitan Transportation Plann...   \n",
       "1  1541707.0  29201300  Southeast Metropolitan Planning Organization (...   \n",
       "2  2101128.0  51197401                                  Richmond Area MPO   \n",
       "3   220693.0  15201300                                           Maui MPO   \n",
       "4   220707.0  15197500                                           Oahu MPO   \n",
       "\n",
       "  STATE STFIPS  STATEFP COUNTYFP  COUNTYNS  GEOID       NAME     ...      \\\n",
       "0    TN     47       47      163   1639793  47163   Sullivan     ...       \n",
       "1    MO     29       17      003    424203  17003  Alexander     ...       \n",
       "2    VA     51       51      670   1498428  51670   Hopewell     ...       \n",
       "3    HI     15       15      009    365283  15009       Maui     ...       \n",
       "4    HI     15       15      003    365281  15003   Honolulu     ...       \n",
       "\n",
       "    CBSAFP  METDIVFP FUNCSTAT         ALAND        AWATER   INTPTLAT  \\\n",
       "0  28700.0       NaN        A  1.070725e+09  4.220920e+07  36.510213   \n",
       "1  16020.0       NaN        A  6.099969e+08  4.423719e+07  37.183657   \n",
       "2  40060.0       NaN        F  2.665508e+07  1.324078e+06  37.291010   \n",
       "3  27980.0       NaN        A -1.286608e+09 -1.091674e+09  20.855931   \n",
       "4  46520.0       NaN        A  1.555505e+09 -2.400273e+08  21.461365   \n",
       "\n",
       "     INTPTLON      PERIMETER   area_sqmi  st_co_fips  \n",
       "0  -82.299396   71837.801334    0.756540       47163  \n",
       "1  -89.349516    6972.897419    0.591829       29003  \n",
       "2  -77.298944    6041.591069    0.257294       51670  \n",
       "3 -156.601550  299148.486465  728.079214       15009  \n",
       "4 -158.201974  329335.691836  610.910815       15003  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dictionary of states and fips codes. \n",
    "fips_state = pd.read_csv(os.path.join(DATA_DIR,'state_fips_codes.csv'),dtype=str)\n",
    "fips2state=dict(zip(fips_state['FIPS'],fips_state['USPS']))\n",
    "state2fips=dict(zip(fips_state['USPS'],fips_state['FIPS']))\n",
    "\n",
    "# Make lookup for county to MPO code \n",
    "mpo_counties = pd.read_csv(os.path.join(DATA_DIR,'us_2015_mpo_regions_counties_v1.csv'), encoding='latin1', dtype={'MPO_ID':str,'COUNTYFP':str,'STFIPS':str})\n",
    "mpo_counties['COUNTYFP'] = mpo_counties['COUNTYFP'].str.zfill(2)  \n",
    "mpo_counties['st_co_fips'] = mpo_counties['STFIPS']+mpo_counties['COUNTYFP']  # we will want to join on 2-char state + 3-char county fips\n",
    "county2mpo=dict(zip(mpo_counties['st_co_fips'],mpo_counties['MPO_ID']))  # do we want MPO_ID or do we want GEOID? \n",
    "mpo_counties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_query(q):\n",
    "    \"\"\" Get results given SQL query\"\"\"\n",
    "    cur.execute(q)\n",
    "    return(cur.fetchall())\n",
    "\n",
    "def get_craiglist(filters, split_fips=True):\n",
    "    \"\"\"Get craiglist data from database.\n",
    "    Args: \n",
    "        filters (list): list of strings containing filter criteria. Format as individual SQL WHERE statements. E.g., [\"region='sandiego'\",\"rent>100\"]\n",
    "        split_fips (bool): if True, split fips code into block and fips12 (useful if merging wtih blockgroup)\n",
    "    Returns: \n",
    "        DataFrame: listings data. \n",
    "    \"\"\"\n",
    "    #q=\"SELECT pid,date,rent,bedrooms,bathrooms,sqft,rent_sqft,fips_block,state,region,longitude,latitude FROM rental_listings WHERE state='{}';\".format(state)\n",
    "    filters_str = ' AND '.join([x for x in filters])\n",
    "    q=\"SELECT pid,date,rent,bedrooms,bathrooms,sqft,rent_sqft,fips_block,state,region,longitude,latitude FROM rental_listings WHERE {};\".format(filters_str)\n",
    "    results=run_query(q)\n",
    "    df=pd.DataFrame(results,columns=['listing_id', 'date','rent','bedrooms','bathrooms','sqft','rent_sqft','fips_block','state','region','lng','lat'] )  # put it all into a dataframe\n",
    "    if split_fips==True:\n",
    "        # split FIPS into different columns - split off the last 3 chars\n",
    "        df['block']=df.fips_block.str[-4:]\n",
    "        df['fips12']=df.fips_block.str[:-3]\n",
    "    return(df)\n",
    "\n",
    "def read_census_file(fname):\n",
    "    \"\"\"Read census csv file via SFTP and return as dataframe.\"\"\"\n",
    "    with sftp.open(os.path.join(CENSUS_DIR,fname)) as f:\n",
    "        df = pd.read_csv(f, delimiter=',',dtype={'age_of_head':float, 'block group':str, 'cars':float, 'children':float, 'county':str,\n",
    "           'household_id':str, 'income':float, 'persons':float, 'race_of_head':str, 'recent_mover':str,\n",
    "           'serialno':str, 'state':str, 'tenure':str, 'tract':str, 'workers':float})\n",
    "    return df\n",
    "\n",
    "def write_results_file(data,fname):\n",
    "    \"\"\"Write merged data to csv file via SFTP.\"\"\"\n",
    "    with sftp.open(os.path.join(RESULTS_DIR,fname),'w') as f:\n",
    "        data.to_csv(f,index=True)\n",
    "    return\n",
    "\n",
    "def get_census_by_state(state, table='households'): \n",
    "    \"\"\"Return all census data for state given two-char abbreviation. Can be 'households' or 'persons' data. \"\"\" \n",
    "    filelist=sftp.listdir(CENSUS_DIR)\n",
    "    if table=='households':\n",
    "        files = [f for f in filelist if f[:5]=='hh_{}'.format(state)]\n",
    "    elif table=='persons':\n",
    "        files = [f for f in filelist if f[:4]=='p_{}'.format(state)]\n",
    "    #files = files[:3]  # uncomment this line for testing.\n",
    "    new_df = pd.DataFrame()\n",
    "    for f in files:\n",
    "        df = read_census_file(f)\n",
    "        new_df = pd.concat([new_df,df])\n",
    "    return(new_df)\n",
    "\n",
    "def strip_zeros(s):\n",
    "    \"\"\"Remove '.0 from end of string\"\"\"\n",
    "    if s.endswith('.0'):\n",
    "        return(s[:-2])\n",
    "    else:\n",
    "        return(s)\n",
    "\n",
    "def format_hh_data(df):\n",
    "    \"\"\"Fix formatting for hhs census data. Replace '' strings with zero. Format other strings.\"\"\"\n",
    "\n",
    "    df['county'] = df['county'].str.zfill(2)  # make county 3-char string.\n",
    "    \n",
    "    for col in ['children','workers']:\n",
    "        df[col] = df[col].replace('','0')\n",
    "\n",
    "    for col in ['race_of_head','recent_mover','tenure']:\n",
    "        df[col] = df[col].astype(str)\n",
    "        df[col] = df[col].map(strip_zeros)  # make sure strings are formatted. \n",
    "    return(df)\n",
    "\n",
    "def aggregate_census(df, groupby_cols=['county','tract','block group'],cols_to_sum=['cars','children','persons','workers'], cols_to_median=['age_of_head','income'],categ_cols=['race_of_head','recent_mover','tenure'],id_col='serialno',table='hhs'):\n",
    "    \"\"\"Aggregate census table to block group. Made this for hh data, may need to revised for persons data.\n",
    "    Args: \n",
    "        groupby_cols (list): names of columns to group by (default=['county','tract','block group'])\n",
    "        cols_to_sum (list): names of columns for which to compute totals. \n",
    "        cols_to_median (list): names of columns for which to compute medians\n",
    "        categ_cols (list): names of categorical columns\n",
    "        id_col (str): name of column that serves as the id column, to use in counting rows. \n",
    "        table (str): 'hhs' (default) or 'per'\n",
    "    Returns: \n",
    "        DataFrame: aggregated data. \n",
    "        \"\"\"\n",
    "    # For some columns we'll want to find the sum or average/median. These will need only a simple groupby \n",
    "    sums = df.groupby(by=groupby_cols).sum()[cols_to_sum]\n",
    "    sums.columns = [x+'_tot' for x in cols_to_sum]\n",
    "    \n",
    "    medians = df.groupby(by=groupby_cols).median()[cols_to_median]\n",
    "    medians.columns = [x+'_med' for x in cols_to_median]\n",
    "    \n",
    "    counts = pd.DataFrame(df.groupby(by=groupby_cols).count()[id_col])\n",
    "    counts.columns=[table+'_tot']\n",
    "\n",
    "    # Categorical columns will need pivot tables. \n",
    "    categoricals = pd.DataFrame(index=counts.index)\n",
    "    for col in categ_cols:\n",
    "        pivoted=df.pivot_table(index = groupby_cols, columns = col, aggfunc='count')[id_col]\n",
    "        pivoted.columns = [col+'_'+x for x in pivoted.columns]\n",
    "        pivoted.columns = pivoted.columns.map(strip_zeros)\n",
    "        # merge back together\n",
    "        categoricals = pd.merge(categoricals, pivoted, left_index=True, right_index=True)\n",
    "\n",
    "    # put all back together in one table\n",
    "    merged = pd.merge(sums, medians, left_index=True, right_index=True)\n",
    "    merged = pd.merge(merged, counts, left_index=True, right_index=True)\n",
    "    merged = pd.merge(merged, categoricals, left_index=True, right_index=True)\n",
    "\n",
    "    # check lengths of dataframes to detect any problems in grouping or merging\n",
    "    lengths = [len(sums),len(medians),len(counts),len(categoricals),len(merged)]\n",
    "    if len(set(lengths))>1:\n",
    "        print('Warning: Aggregated tables have different lengths.',lengths,'for sums, medians, counts, categoricals, and merged.')\n",
    "    \n",
    "    return(merged)\n",
    "\n",
    "def match_mpo(s, mpo_dict=county2mpo):\n",
    "    \"\"\"Match a 5-char state-county FIPS code to an MPO code\n",
    "    Args: \n",
    "        s (str): 5-char state-county string\n",
    "        mpo_dict (dict): county2mpo dictionary\n",
    "    Returns: \n",
    "        str: MPO code\n",
    "    \"\"\"\n",
    "    try: \n",
    "        return mpo_dict[s]\n",
    "    except KeyError: # in this case, the county is not in an MPO\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_all(state, filters=None):\n",
    "    \"\"\"Get craigslist data and merge with census data, by state, and save.  with additional filters if needed. \n",
    "    Args: \n",
    "        state (str): 2-char state abbreviation\n",
    "        filters (list): additional filters. Do not need to include state in filter list\n",
    "    \"\"\"\n",
    "    \n",
    "    # load and prepare craiglist data\n",
    "    \n",
    "    # If filters are provided, use them to filter data\n",
    "    if filters:\n",
    "        filters.append(\"state='{}'\".format(state))\n",
    "        print(filters)\n",
    "        df_cl=get_craiglist(filters)\n",
    "    # If no filters provided, get all data for the specified state. \n",
    "    else:\n",
    "        df_cl=get_craiglist([\"state='{}'\".format(state)])\n",
    "    \n",
    "    df_cl['st_co_fps'] = df_cl.fips_block.map(lambda x: x[:5])\n",
    "    df_cl['mpo_id'] = df_cl.st_co_fps.map(match_mpo)\n",
    "\n",
    "    # load and prepare census data for households\n",
    "    hhs = get_census_by_state(state, table='households')\n",
    "    hhs = format_hh_data(hhs)\n",
    "    hhs_bg = aggregate_census(hhs)\n",
    "    hhs_bg=hhs_bg.reset_index()\n",
    "    hhs_bg['fips12']=state2fips[state]+hhs_bg['county']+hhs_bg['tract']+hhs_bg['block group'] # create 12-digit FIPS code for merging. \n",
    "\n",
    "    # merge with craigslist data. \n",
    "    merged = pd.merge(df_cl, hhs_bg, on='fips12',how='left')\n",
    "    merged = merged.set_index('listing_id')\n",
    "\n",
    "    #TODO: add persons data here, if needed. \n",
    "\n",
    "    # Keep only columns we'll need.\n",
    "    cols_to_keep=['date','rent','bedrooms','bathrooms','sqft','rent_sqft','fips_block','state','region','mpo_id','lng','lat','cars_tot','children_tot','persons_tot','workers_tot','age_of_head_med','income_med','hhs_tot','race_of_head_1','race_of_head_2','race_of_head_3','race_of_head_4','race_of_head_5','race_of_head_6','race_of_head_7','race_of_head_8','race_of_head_9','recent_mover_0','recent_mover_1','tenure_1','tenure_2']\n",
    "    \n",
    "    # This is a bit of a hack in case some columns are missing in some states. \n",
    "    for col in cols_to_keep: \n",
    "        if col not in merged.columns:\n",
    "            merged[col] = np.nan\n",
    "\n",
    "    # save file either locally or remotely. \n",
    "    print('Saving data for {s}: {m} rows'.format(s=state,m=len(merged)))\n",
    "    outfile = 'cl_census_{}.csv'.format(state)\n",
    "    #merged[cols_to_keep].to_csv(os.path.join(DATA_DIR,outfile), index=True)  # uncomment to save locally\n",
    "\n",
    "    #write_results_file(merged[cols_to_keep], outfile)  # uncomment to save remotely. \n",
    "    return merged[cols_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data for a single region\n",
    "\n",
    "Use this to get data for a single region, for use in our preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"region = 'sfbay'\", 'rent>0', \"state='CA'\"]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3f9d58715a9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_bayarea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"region = 'sfbay'\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rent>0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# define whatever filters you want here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_bayarea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f4f50040be1e>\u001b[0m in \u001b[0;36mrun_all\u001b[0;34m(state, filters)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# load and prepare census data for households\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mhhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_census_by_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'households'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mhhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_hh_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mhhs_bg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate_census\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d0074092a625>\u001b[0m in \u001b[0;36mget_census_by_state\u001b[0;34m(state, table)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_census_by_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'households'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;34m\"\"\"Return all census data for state given two-char abbreviation. Can be 'households' or 'persons' data. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mfilelist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msftp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCENSUS_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'households'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilelist\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'hh_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/paramiko/sftp_client.py\u001b[0m in \u001b[0;36mlistdir\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mstr\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mto\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \"\"\"\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlistdir_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/paramiko/sftp_client.py\u001b[0m in \u001b[0;36mlistdir_attr\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCMD_READDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;31m# done with handle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/paramiko/sftp_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, t, *arg)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/paramiko/sftp_client.py\u001b[0m in \u001b[0;36m_read_response\u001b[0;34m(self, waitfor)\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSHException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Server connection dropped: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/paramiko/sftp.py\u001b[0m in \u001b[0;36m_read_packet\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;31m# most sftp servers won't accept packets larger than about 32k, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# anything with the high byte set (> 16MB) is just garbage.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/paramiko/sftp.py\u001b[0m in \u001b[0;36m_read_all\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    154\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/paramiko/channel.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, nbytes)\u001b[0m\n\u001b[1;32m    663\u001b[0m         \"\"\"\n\u001b[1;32m    664\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPipeTimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/paramiko/buffered_pipe.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nbytes, timeout)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mthen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                         \u001b[0mtimeout\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mthen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_bayarea = run_all(state='CA',filters=[\"region = 'sfbay'\",\"rent>0\"])   # define whatever filters you want here.\n",
    "df_bayarea.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save locally\n",
    "outfile = 'sfbay_listings_04282017.csv'\n",
    "df_bayarea.to_csv(os.path.join(DATA_DIR,outfile), index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all data by state\n",
    "Use this to merge all the data by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for state in fips_state['USPS']:# uncomment when done with testing. \n",
    "    if state != 'DC':   # the DC census data is missing. \n",
    "        print('\\n Working on',state)\n",
    "        df_state = run_all(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Merge listings data with urbansim block-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first unzip csv file into temp dir\n",
    "os.mkdir('temp') # make temp dir for unzipped files\n",
    "zip_ref = zipfile.ZipFile(os.path.join(BLOCK_DIR,BLOCK_ZFILE), 'r')\n",
    "zip_ref.extractall('temp')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1015)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>res_rents</th>\n",
       "      <th>res_values</th>\n",
       "      <th>square_meters_land</th>\n",
       "      <th>puma10_id</th>\n",
       "      <th>residential_unit_capacity</th>\n",
       "      <th>employment_capacity</th>\n",
       "      <th>rent_impute</th>\n",
       "      <th>...</th>\n",
       "      <th>tracts_prop_persons_8</th>\n",
       "      <th>tracts_prop_persons_9</th>\n",
       "      <th>nodes_du_1500m</th>\n",
       "      <th>nodes_ave_year_built_3000m</th>\n",
       "      <th>tracts_prop_persons_5</th>\n",
       "      <th>tracts_prop_persons_6</th>\n",
       "      <th>tracts_prop_persons_7</th>\n",
       "      <th>tracts_prop_persons_1</th>\n",
       "      <th>tracts_prop_persons_2</th>\n",
       "      <th>tracts_prop_persons_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>060014271001000</td>\n",
       "      <td>-122.233867</td>\n",
       "      <td>37.770270</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>777700.0</td>\n",
       "      <td>0</td>\n",
       "      <td>600105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.241375</td>\n",
       "      <td>1949.245239</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.014378</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.244428</td>\n",
       "      <td>0.35514</td>\n",
       "      <td>0.180446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>060014271001001</td>\n",
       "      <td>-122.233991</td>\n",
       "      <td>37.769464</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>567900.0</td>\n",
       "      <td>79696</td>\n",
       "      <td>600105</td>\n",
       "      <td>105</td>\n",
       "      <td>367</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.241375</td>\n",
       "      <td>1949.245239</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.014378</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.244428</td>\n",
       "      <td>0.35514</td>\n",
       "      <td>0.180446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>060014271001002</td>\n",
       "      <td>-122.234301</td>\n",
       "      <td>37.768350</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>777700.0</td>\n",
       "      <td>739</td>\n",
       "      <td>600105</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.241375</td>\n",
       "      <td>1949.245239</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.014378</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.244428</td>\n",
       "      <td>0.35514</td>\n",
       "      <td>0.180446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>060014271001003</td>\n",
       "      <td>-122.235213</td>\n",
       "      <td>37.768827</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>567900.0</td>\n",
       "      <td>19546</td>\n",
       "      <td>600105</td>\n",
       "      <td>48</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.475714</td>\n",
       "      <td>1949.422974</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.014378</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.244428</td>\n",
       "      <td>0.35514</td>\n",
       "      <td>0.180446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>060014271001004</td>\n",
       "      <td>-122.236751</td>\n",
       "      <td>37.769734</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>567900.0</td>\n",
       "      <td>14364</td>\n",
       "      <td>600105</td>\n",
       "      <td>35</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.157094</td>\n",
       "      <td>1950.110962</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.014378</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.244428</td>\n",
       "      <td>0.35514</td>\n",
       "      <td>0.180446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1015 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          block_id           x          y  res_rents  res_values  \\\n",
       "0  060014271001000 -122.233867  37.770270     1475.0    777700.0   \n",
       "1  060014271001001 -122.233991  37.769464     1253.0    567900.0   \n",
       "2  060014271001002 -122.234301  37.768350     1475.0    777700.0   \n",
       "3  060014271001003 -122.235213  37.768827     1253.0    567900.0   \n",
       "4  060014271001004 -122.236751  37.769734     1253.0    567900.0   \n",
       "\n",
       "   square_meters_land  puma10_id  residential_unit_capacity  \\\n",
       "0                   0     600105                          0   \n",
       "1               79696     600105                        105   \n",
       "2                 739     600105                          0   \n",
       "3               19546     600105                         48   \n",
       "4               14364     600105                         35   \n",
       "\n",
       "   employment_capacity  rent_impute          ...            \\\n",
       "0                    0            1          ...             \n",
       "1                  367            0          ...             \n",
       "2                   23            1          ...             \n",
       "3                  108            0          ...             \n",
       "4                  105            0          ...             \n",
       "\n",
       "   tracts_prop_persons_8  tracts_prop_persons_9  nodes_du_1500m  \\\n",
       "0                    0.0                    0.0        7.241375   \n",
       "1                    0.0                    0.0        7.241375   \n",
       "2                    0.0                    0.0        7.241375   \n",
       "3                    0.0                    0.0        7.475714   \n",
       "4                    0.0                    0.0        7.157094   \n",
       "\n",
       "   nodes_ave_year_built_3000m  tracts_prop_persons_5  tracts_prop_persons_6  \\\n",
       "0                 1949.245239               0.033789               0.014378   \n",
       "1                 1949.245239               0.033789               0.014378   \n",
       "2                 1949.245239               0.033789               0.014378   \n",
       "3                 1949.422974               0.033789               0.014378   \n",
       "4                 1950.110962               0.033789               0.014378   \n",
       "\n",
       "   tracts_prop_persons_7  tracts_prop_persons_1  tracts_prop_persons_2  \\\n",
       "0               0.007189               0.244428                0.35514   \n",
       "1               0.007189               0.244428                0.35514   \n",
       "2               0.007189               0.244428                0.35514   \n",
       "3               0.007189               0.244428                0.35514   \n",
       "4               0.007189               0.244428                0.35514   \n",
       "\n",
       "   tracts_prop_persons_3  \n",
       "0               0.180446  \n",
       "1               0.180446  \n",
       "2               0.180446  \n",
       "3               0.180446  \n",
       "4               0.180446  \n",
       "\n",
       "[5 rows x 1015 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temporarily read first 100 lines just to see header names\n",
    "df_temp = pd.read_csv(os.path.join('temp',BLOCK_FILE), nrows=100, dtype={'block_id':str})\n",
    "print(df_temp.shape)\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the columns we need \n",
    "block_cols = df_temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_temp.columns[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain columns are definitely not useful. E.g, 'puma10_id_is_0609502', 'puma10_id_is_0609503'\n",
    "'tracts_mean_y', 'tracts_mean_x'\n",
    "\n",
    "TODO: spend more time choosing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-c056f67af7df>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-c056f67af7df>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    unneeded_cols = [x for x in block_cols if x.startswith('puma10_id_is_')]+\u001b[0m\n\u001b[0m                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# make sure to now include unneeded columns like these:\n",
    "unneeded_cols = [x for x in block_cols if x.startswith('puma10_id_is_')]+\n",
    "[x for x in block_cols if (x.endswith('mean_y'))|(x.endswith('mean_x'))]+\n",
    "[x for x in block_cols if (x.endswith('std_y'))|(x.endswith('std_x'))]+\n",
    "[x for x in block_cols if x.startswith('pumas_prop_sector_id')]+\n",
    "[x for x in block_cols if x.startswith('county_id_is_')]+\n",
    "[x for x in block_cols if x.startswith('tracts_prop_sector_id')]+\n",
    "[x for x in block_cols if x.startswith('counties_prop_sector_id')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(unneeded_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_to_use = ['block_id','nodes_population_1500m','block_groups_total_jobs', 'block_groups_median_children', \n",
    "       'block_groups_median_income', 'prop_tenure_1', 'nodes_low_income_hh_1500m', 'nodes_high_income_hh_1500m', \n",
    "       'nodes_jobs_3000m','nodes_jobs_20km', 'nodes_population_400m', 'nodes_population_800m', \n",
    "       'block_groups_prop_race_of_head_1','block_groups_prop_race_of_head_2', 'block_groups_prop_race_of_head_3', \n",
    "       'block_groups_prop_race_of_head_7','block_groups_prop_race_of_head_8','block_groups_prop_race_of_head_6',\n",
    "       'pumas_density_residential_units','block_groups_density_jobs', \n",
    "       'nodes_jobs_1500m_4445','nodes_jobs_3000m_4445', \n",
    "       'nodes_du_5000m','nodes_du_800m','block_groups_median_rent',\n",
    "       'block_groups_median_persons', 'block_groups_median_age_of_head', 'nodes_ave_year_built_800m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in cols_to_use: \n",
    "    if col not in block_cols:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109228, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block_id</th>\n",
       "      <th>nodes_low_income_hh_1500m</th>\n",
       "      <th>block_groups_median_children</th>\n",
       "      <th>nodes_jobs_3000m_4445</th>\n",
       "      <th>block_groups_total_jobs</th>\n",
       "      <th>nodes_population_1500m</th>\n",
       "      <th>prop_tenure_1</th>\n",
       "      <th>block_groups_median_persons</th>\n",
       "      <th>block_groups_median_age_of_head</th>\n",
       "      <th>nodes_population_800m</th>\n",
       "      <th>...</th>\n",
       "      <th>nodes_du_5000m</th>\n",
       "      <th>nodes_jobs_1500m_4445</th>\n",
       "      <th>pumas_density_residential_units</th>\n",
       "      <th>nodes_jobs_3000m</th>\n",
       "      <th>block_groups_median_rent</th>\n",
       "      <th>nodes_jobs_20km</th>\n",
       "      <th>block_groups_density_jobs</th>\n",
       "      <th>block_groups_median_income</th>\n",
       "      <th>nodes_population_400m</th>\n",
       "      <th>nodes_du_800m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>060014271001000</td>\n",
       "      <td>5.991451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.705032</td>\n",
       "      <td>465.0</td>\n",
       "      <td>9.113499</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7.809947</td>\n",
       "      <td>...</td>\n",
       "      <td>9.915149</td>\n",
       "      <td>5.062971</td>\n",
       "      <td>3.339906</td>\n",
       "      <td>8.821410</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>12.139385</td>\n",
       "      <td>5.103936</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>6.373320</td>\n",
       "      <td>5.841072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>060014271001001</td>\n",
       "      <td>5.991451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.705032</td>\n",
       "      <td>465.0</td>\n",
       "      <td>9.113499</td>\n",
       "      <td>0.200</td>\n",
       "      <td>2.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7.809947</td>\n",
       "      <td>...</td>\n",
       "      <td>9.915149</td>\n",
       "      <td>5.062971</td>\n",
       "      <td>3.339906</td>\n",
       "      <td>8.821410</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>12.139385</td>\n",
       "      <td>5.103936</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>6.373320</td>\n",
       "      <td>5.841072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>060014271001002</td>\n",
       "      <td>5.991451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.705032</td>\n",
       "      <td>465.0</td>\n",
       "      <td>9.113499</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7.809947</td>\n",
       "      <td>...</td>\n",
       "      <td>9.915149</td>\n",
       "      <td>5.062971</td>\n",
       "      <td>3.339906</td>\n",
       "      <td>8.821410</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>12.139385</td>\n",
       "      <td>5.103936</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>6.373320</td>\n",
       "      <td>5.841072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>060014271001003</td>\n",
       "      <td>6.231131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.814257</td>\n",
       "      <td>465.0</td>\n",
       "      <td>9.536473</td>\n",
       "      <td>0.300</td>\n",
       "      <td>2.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7.960672</td>\n",
       "      <td>...</td>\n",
       "      <td>9.982758</td>\n",
       "      <td>5.257902</td>\n",
       "      <td>3.339906</td>\n",
       "      <td>8.925516</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>12.150690</td>\n",
       "      <td>5.103936</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>6.703188</td>\n",
       "      <td>6.253922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>060014271001004</td>\n",
       "      <td>6.107350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.982764</td>\n",
       "      <td>465.0</td>\n",
       "      <td>9.355220</td>\n",
       "      <td>0.375</td>\n",
       "      <td>2.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7.547502</td>\n",
       "      <td>...</td>\n",
       "      <td>10.062321</td>\n",
       "      <td>5.541484</td>\n",
       "      <td>3.339906</td>\n",
       "      <td>9.048043</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>12.218818</td>\n",
       "      <td>5.103936</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>6.396930</td>\n",
       "      <td>5.760618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          block_id  nodes_low_income_hh_1500m  block_groups_median_children  \\\n",
       "0  060014271001000                   5.991451                           0.0   \n",
       "1  060014271001001                   5.991451                           0.0   \n",
       "2  060014271001002                   5.991451                           0.0   \n",
       "3  060014271001003                   6.231131                           0.0   \n",
       "4  060014271001004                   6.107350                           0.0   \n",
       "\n",
       "   nodes_jobs_3000m_4445  block_groups_total_jobs  nodes_population_1500m  \\\n",
       "0               6.705032                    465.0                9.113499   \n",
       "1               6.705032                    465.0                9.113499   \n",
       "2               6.705032                    465.0                9.113499   \n",
       "3               6.814257                    465.0                9.536473   \n",
       "4               6.982764                    465.0                9.355220   \n",
       "\n",
       "   prop_tenure_1  block_groups_median_persons  \\\n",
       "0          0.000                          2.0   \n",
       "1          0.200                          2.0   \n",
       "2          0.000                          2.0   \n",
       "3          0.300                          2.0   \n",
       "4          0.375                          2.0   \n",
       "\n",
       "   block_groups_median_age_of_head  nodes_population_800m      ...        \\\n",
       "0                             57.0               7.809947      ...         \n",
       "1                             57.0               7.809947      ...         \n",
       "2                             57.0               7.809947      ...         \n",
       "3                             57.0               7.960672      ...         \n",
       "4                             57.0               7.547502      ...         \n",
       "\n",
       "   nodes_du_5000m  nodes_jobs_1500m_4445  pumas_density_residential_units  \\\n",
       "0        9.915149               5.062971                         3.339906   \n",
       "1        9.915149               5.062971                         3.339906   \n",
       "2        9.915149               5.062971                         3.339906   \n",
       "3        9.982758               5.257902                         3.339906   \n",
       "4       10.062321               5.541484                         3.339906   \n",
       "\n",
       "   nodes_jobs_3000m  block_groups_median_rent  nodes_jobs_20km  \\\n",
       "0          8.821410                    1253.0        12.139385   \n",
       "1          8.821410                    1253.0        12.139385   \n",
       "2          8.821410                    1253.0        12.139385   \n",
       "3          8.925516                    1253.0        12.150690   \n",
       "4          9.048043                    1253.0        12.218818   \n",
       "\n",
       "   block_groups_density_jobs  block_groups_median_income  \\\n",
       "0                   5.103936                     80000.0   \n",
       "1                   5.103936                     80000.0   \n",
       "2                   5.103936                     80000.0   \n",
       "3                   5.103936                     80000.0   \n",
       "4                   5.103936                     80000.0   \n",
       "\n",
       "   nodes_population_400m  nodes_du_800m  \n",
       "0               6.373320       5.841072  \n",
       "1               6.373320       5.841072  \n",
       "2               6.373320       5.841072  \n",
       "3               6.703188       6.253922  \n",
       "4               6.396930       5.760618  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   \n",
    "# Read all rows, using only the columns we want\n",
    "df_blocks = pd.read_csv(os.path.join('temp',BLOCK_FILE),dtype={'block_id':str}, usecols = cols_to_use)\n",
    "print(df_blocks.shape)\n",
    "df_blocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    060014271001000\n",
       "1    060014271001001\n",
       "2    060014271001002\n",
       "3    060014271001003\n",
       "4    060014271001004\n",
       "Name: block_id, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_blocks['block_id'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_listings = get_craiglist(filters = [\"region='sfbay'\",\"rent>100\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125560 109228\n",
      "Warning: only 123463 of 125560 rows matched\n"
     ]
    }
   ],
   "source": [
    "# merge listings with vars on block_id\n",
    "df_listings.fips_block.head()\n",
    "\n",
    "print(len(df_listings), len(df_blocks))\n",
    "df_merged = pd.merge(df_listings, df_blocks, left_on='fips_block', right_on='block_id', how='inner')\n",
    "if len(df_merged)<len(df_listings):\n",
    "    print('Warning: only {0} of {1} rows matched'.format(len(df_merged), len(df_listings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/pandas/core/generic.py:1138: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block1_values] [items->['listing_id', 'date', 'fips_block', 'state', 'region', 'block', 'fips12', 'block_id']]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# save to hdf\n",
    "outfile = 'ba_listings.h5'\n",
    "df_merged.to_hdf(os.path.join(DATA_DIR,outfile),'merged')\n",
    "\n",
    "\n",
    "outfile = 'ba_listings.csv'\n",
    "df_merged.to_csv(os.path.join(DATA_DIR,outfile),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store data to database\n",
    "\n",
    "We need to create a database table to hold the block-level variables for the rent-predictor app. \n",
    "\n",
    "\n",
    "Steps: \n",
    "\n",
    "1. Create table schema\n",
    "\n",
    "Assume all features are floats unless otherwise specified. \n",
    "\n",
    "(If have a lot that aren't floats, might want to store the variable names and their types as a json so we can refer back to it when we change the variables.)\n",
    "\n",
    "2. Copy data into table\n",
    "\n",
    "Copy data into table. COPY is the fastest way to insert a large amount of data (https://www.postgresql.org/docs/current/static/populate.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to local databse\n",
    "\n",
    "DBNAME = settings['DBNAME_RENT']\n",
    "# USER = settings['USER_RENT']\n",
    "# PASSWORD = settings['PASSWORD_RENT']\n",
    "\n",
    "conn_str = \"dbname = {0}\".format(DBNAME)\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(conn_str)\n",
    "    cur = conn.cursor()\n",
    "except:\n",
    "    print (\"Cannot connection. Check settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first save data as csv. \n",
    "# let's use that temp dir again\n",
    "FULL_PATH = '/Users/lisarayle/Dropbox/craigslist/src/'  # can't use relative path in postgres, I guess\n",
    "csvfile = 'blocks_temp.csv'\n",
    "df_blocks.to_csv(os.path.join('temp',csvfile), index=False)\n",
    "\n",
    "table_name = 'block_vars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_db_table(col_names,t_name,id_var='block_id'):\n",
    "    \"\"\"Create a new table with schema to hold the block-level data. \n",
    "    Args: \n",
    "        col_names (list): list of names of columns to use. First one can be 'block id'\n",
    "        t_name (str): name of database table\n",
    "        id_var (str): name of id variable (default: 'block_id')\n",
    "    \"\"\"\n",
    "    # drop table if already exists\n",
    "    q = \"DROP TABLE IF EXISTS {}\".format(t_name)\n",
    "    cur.execute(q)\n",
    "    conn.commit()\n",
    "\n",
    "    # build the SQL string\n",
    "    sql_begin = \"CREATE TABLE {0} (id BIGSERIAL PRIMARY KEY, {1} varchar(15) not null, \".format(t_name, id_var)\n",
    "    if col_names[0]==id_var:\n",
    "        sql_middle = \" real,\".join([c for c in col_names[1:]]) # leave off block id if it's there. \n",
    "    else: \n",
    "        sql_middle = \" real,\".join([c for c in col_names])\n",
    "\n",
    "    sql_end = \" real);\"\n",
    "    q = sql_begin+sql_middle+sql_end\n",
    "    \n",
    "    cur.execute(q)\n",
    "    conn.commit()\n",
    "    return \n",
    "\n",
    "def copy_block_data(col_names,t_name,fname):\n",
    "    \"\"\"Copy data from csv file into block variables table. \n",
    "    Args: \n",
    "        col_names (list): list of names of columns to use. First one can be 'block id'\n",
    "        t_name (str): name of database table\n",
    "        fname (str): name of csv file with data\n",
    "    \"\"\"\n",
    "    var_string = ','.join([c for c in col_names])\n",
    "    q=\"COPY {t}({v}) FROM '{f}' DELIMITERS ',' CSV HEADER;\".format(t=t_name,v=var_string, f=os.path.join(FULL_PATH,'temp',fname))\n",
    "    print(q)\n",
    "    cur.execute(q)\n",
    "    conn.commit()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPY block_vars(block_id,nodes_population_1500m,block_groups_total_jobs,block_groups_median_children,block_groups_median_income,prop_tenure_1,nodes_low_income_hh_1500m,nodes_high_income_hh_1500m,nodes_jobs_3000m,nodes_jobs_20km,nodes_population_400m,nodes_population_800m,block_groups_prop_race_of_head_1,block_groups_prop_race_of_head_2,block_groups_prop_race_of_head_3,block_groups_prop_race_of_head_7,block_groups_prop_race_of_head_8,block_groups_prop_race_of_head_6,pumas_density_residential_units,block_groups_density_jobs,nodes_jobs_1500m_4445,nodes_jobs_3000m_4445,nodes_du_5000m,nodes_du_800m,block_groups_median_rent,block_groups_median_persons,block_groups_median_age_of_head,nodes_ave_year_built_800m) FROM '/Users/lisarayle/Dropbox/craigslist/src/temp/blocks_temp.csv' DELIMITERS ',' CSV HEADER;\n"
     ]
    }
   ],
   "source": [
    "create_db_table(cols_to_use, table_name)\n",
    "copy_block_data(cols_to_use, table_name, csvfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  '060014271001000',\n",
       "  5.99145,\n",
       "  0.0,\n",
       "  6.70503,\n",
       "  465.0,\n",
       "  9.1135,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  57.0,\n",
       "  7.80995,\n",
       "  6.1145,\n",
       "  0.109726,\n",
       "  0.0,\n",
       "  0.00498753,\n",
       "  0.0,\n",
       "  0.885287,\n",
       "  0.0,\n",
       "  1940.73,\n",
       "  9.91515,\n",
       "  5.06297,\n",
       "  3.33991,\n",
       "  8.82141,\n",
       "  1253.0,\n",
       "  12.1394,\n",
       "  5.10394,\n",
       "  80000.0,\n",
       "  6.37332,\n",
       "  5.84107),\n",
       " (2,\n",
       "  '060014271001001',\n",
       "  5.99145,\n",
       "  0.0,\n",
       "  6.70503,\n",
       "  465.0,\n",
       "  9.1135,\n",
       "  0.2,\n",
       "  2.0,\n",
       "  57.0,\n",
       "  7.80995,\n",
       "  6.1145,\n",
       "  0.109726,\n",
       "  0.0,\n",
       "  0.00498753,\n",
       "  0.0,\n",
       "  0.885287,\n",
       "  0.0,\n",
       "  1940.73,\n",
       "  9.91515,\n",
       "  5.06297,\n",
       "  3.33991,\n",
       "  8.82141,\n",
       "  1253.0,\n",
       "  12.1394,\n",
       "  5.10394,\n",
       "  80000.0,\n",
       "  6.37332,\n",
       "  5.84107),\n",
       " (3,\n",
       "  '060014271001002',\n",
       "  5.99145,\n",
       "  0.0,\n",
       "  6.70503,\n",
       "  465.0,\n",
       "  9.1135,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  57.0,\n",
       "  7.80995,\n",
       "  6.1145,\n",
       "  0.109726,\n",
       "  0.0,\n",
       "  0.00498753,\n",
       "  0.0,\n",
       "  0.885287,\n",
       "  0.0,\n",
       "  1940.73,\n",
       "  9.91515,\n",
       "  5.06297,\n",
       "  3.33991,\n",
       "  8.82141,\n",
       "  1253.0,\n",
       "  12.1394,\n",
       "  5.10394,\n",
       "  80000.0,\n",
       "  6.37332,\n",
       "  5.84107),\n",
       " (4,\n",
       "  '060014271001003',\n",
       "  6.23113,\n",
       "  0.0,\n",
       "  6.81426,\n",
       "  465.0,\n",
       "  9.53647,\n",
       "  0.3,\n",
       "  2.0,\n",
       "  57.0,\n",
       "  7.96067,\n",
       "  6.33903,\n",
       "  0.109726,\n",
       "  0.0,\n",
       "  0.00498753,\n",
       "  0.0,\n",
       "  0.885287,\n",
       "  0.0,\n",
       "  1940.53,\n",
       "  9.98276,\n",
       "  5.2579,\n",
       "  3.33991,\n",
       "  8.92552,\n",
       "  1253.0,\n",
       "  12.1507,\n",
       "  5.10394,\n",
       "  80000.0,\n",
       "  6.70319,\n",
       "  6.25392),\n",
       " (5,\n",
       "  '060014271001004',\n",
       "  6.10735,\n",
       "  0.0,\n",
       "  6.98276,\n",
       "  465.0,\n",
       "  9.35522,\n",
       "  0.375,\n",
       "  2.0,\n",
       "  57.0,\n",
       "  7.5475,\n",
       "  5.76372,\n",
       "  0.109726,\n",
       "  0.0,\n",
       "  0.00498753,\n",
       "  0.0,\n",
       "  0.885287,\n",
       "  0.0,\n",
       "  1956.93,\n",
       "  10.0623,\n",
       "  5.54148,\n",
       "  3.33991,\n",
       "  9.04804,\n",
       "  1253.0,\n",
       "  12.2188,\n",
       "  5.10394,\n",
       "  80000.0,\n",
       "  6.39693,\n",
       "  5.76062),\n",
       " (6,\n",
       "  '060014271001005',\n",
       "  6.18346,\n",
       "  0.0,\n",
       "  6.97409,\n",
       "  465.0,\n",
       "  9.36529,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  57.0,\n",
       "  7.55433,\n",
       "  5.85112,\n",
       "  0.109726,\n",
       "  0.0,\n",
       "  0.00498753,\n",
       "  0.0,\n",
       "  0.885287,\n",
       "  0.0,\n",
       "  1948.95,\n",
       "  10.0493,\n",
       "  5.5048,\n",
       "  3.33991,\n",
       "  9.04969,\n",
       "  1253.0,\n",
       "  12.2108,\n",
       "  5.10394,\n",
       "  80000.0,\n",
       "  6.36819,\n",
       "  5.80167),\n",
       " (7,\n",
       "  '060014271001006',\n",
       "  6.18346,\n",
       "  0.0,\n",
       "  6.97409,\n",
       "  465.0,\n",
       "  9.36529,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  57.0,\n",
       "  7.55433,\n",
       "  5.85112,\n",
       "  0.109726,\n",
       "  0.0,\n",
       "  0.00498753,\n",
       "  0.0,\n",
       "  0.885287,\n",
       "  0.0,\n",
       "  1948.95,\n",
       "  10.0493,\n",
       "  5.5048,\n",
       "  3.33991,\n",
       "  9.04969,\n",
       "  1253.0,\n",
       "  12.2108,\n",
       "  5.10394,\n",
       "  80000.0,\n",
       "  6.36819,\n",
       "  5.80167),\n",
       " (8,\n",
       "  '060014271001007',\n",
       "  6.23485,\n",
       "  0.0,\n",
       "  6.96965,\n",
       "  465.0,\n",
       "  9.42173,\n",
       "  0.666667,\n",
       "  2.0,\n",
       "  57.0,\n",
       "  7.50163,\n",
       "  5.90052,\n",
       "  0.109726,\n",
       "  0.0,\n",
       "  0.00498753,\n",
       "  0.0,\n",
       "  0.885287,\n",
       "  0.0,\n",
       "  1948.78,\n",
       "  10.0434,\n",
       "  5.48594,\n",
       "  3.33991,\n",
       "  9.05139,\n",
       "  1253.0,\n",
       "  12.2067,\n",
       "  5.10394,\n",
       "  80000.0,\n",
       "  5.80212,\n",
       "  5.85131),\n",
       " (9,\n",
       "  '060014271001008',\n",
       "  5.98591,\n",
       "  0.0,\n",
       "  6.88824,\n",
       "  465.0,\n",
       "  9.30356,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  57.0,\n",
       "  7.96763,\n",
       "  6.2831,\n",
       "  0.109726,\n",
       "  0.0,\n",
       "  0.00498753,\n",
       "  0.0,\n",
       "  0.885287,\n",
       "  0.0,\n",
       "  1940.3,\n",
       "  10.0533,\n",
       "  5.32579,\n",
       "  3.33991,\n",
       "  8.96883,\n",
       "  1253.0,\n",
       "  12.1763,\n",
       "  5.10394,\n",
       "  80000.0,\n",
       "  6.56527,\n",
       "  5.99314),\n",
       " (10,\n",
       "  '060014271001009',\n",
       "  5.90645,\n",
       "  0.0,\n",
       "  6.89457,\n",
       "  465.0,\n",
       "  9.34592,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  57.0,\n",
       "  7.86864,\n",
       "  6.17115,\n",
       "  0.109726,\n",
       "  0.0,\n",
       "  0.00498753,\n",
       "  0.0,\n",
       "  0.885287,\n",
       "  0.0,\n",
       "  1940.38,\n",
       "  10.0666,\n",
       "  5.33424,\n",
       "  3.33991,\n",
       "  8.96128,\n",
       "  1253.0,\n",
       "  12.1815,\n",
       "  5.10394,\n",
       "  80000.0,\n",
       "  5.98645,\n",
       "  5.66477)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test queries\n",
    "\n",
    "q = \"select count(*) from block_vars;\"\n",
    "run_query(q)\n",
    "\n",
    "q = \"select column_name from information_schema.columns where table_name='block_vars';\"\n",
    "run_query(q)\n",
    "\n",
    "q = \"select * from block_vars limit 10;\"\n",
    "run_query(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['block_id',\n",
       " 'nodes_population_1500m',\n",
       " 'block_groups_total_jobs',\n",
       " 'block_groups_median_children',\n",
       " 'block_groups_median_income',\n",
       " 'prop_tenure_1',\n",
       " 'nodes_low_income_hh_1500m',\n",
       " 'nodes_high_income_hh_1500m',\n",
       " 'nodes_jobs_3000m',\n",
       " 'nodes_jobs_20km',\n",
       " 'nodes_population_400m',\n",
       " 'nodes_population_800m',\n",
       " 'block_groups_prop_race_of_head_1',\n",
       " 'block_groups_prop_race_of_head_2',\n",
       " 'block_groups_prop_race_of_head_3',\n",
       " 'block_groups_prop_race_of_head_7',\n",
       " 'block_groups_prop_race_of_head_8',\n",
       " 'block_groups_prop_race_of_head_6',\n",
       " 'pumas_density_residential_units',\n",
       " 'block_groups_density_jobs',\n",
       " 'nodes_jobs_1500m_4445',\n",
       " 'nodes_jobs_3000m_4445',\n",
       " 'nodes_du_5000m',\n",
       " 'nodes_du_800m',\n",
       " 'block_groups_median_rent',\n",
       " 'block_groups_median_persons',\n",
       " 'block_groups_median_age_of_head',\n",
       " 'nodes_ave_year_built_800m']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cols_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nodes_population_1500m', 'block_groups_total_jobs', 'block_groups_median_children', 'block_groups_median_income', 'prop_tenure_1', 'nodes_low_income_hh_1500m', 'nodes_high_income_hh_1500m', 'nodes_jobs_3000m', 'nodes_jobs_20km', 'nodes_population_400m', 'nodes_population_800m', 'block_groups_prop_race_of_head_1', 'block_groups_prop_race_of_head_2', 'block_groups_prop_race_of_head_3', 'block_groups_prop_race_of_head_7', 'block_groups_prop_race_of_head_8', 'block_groups_prop_race_of_head_6', 'pumas_density_residential_units', 'block_groups_density_jobs', 'nodes_jobs_1500m_4445', 'nodes_jobs_3000m_4445', 'nodes_du_5000m', 'nodes_du_800m', 'block_groups_median_rent', 'block_groups_median_persons', 'block_groups_median_age_of_head', 'nodes_ave_year_built_800m']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR='/Users/lisarayle/rent_predictor/data/'\n",
    "\"\"\"Path to data directory\"\"\"\n",
    "\n",
    "# read file with variable names \n",
    "infile = 'variables.txt'\n",
    "with open(os.path.join(DATA_DIR, infile), 'r') as f:\n",
    "    VARLIST = f.read().split(',')\n",
    "print(VARLIST)\n",
    "len(VARLIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
