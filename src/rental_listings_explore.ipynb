{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import paramiko\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for exploring the merged craigslist/census data and fitting some initial models.\n",
    "\n",
    "TODO:\n",
    "- parse dates and filter for date\n",
    "- look at data from multiple states together\n",
    "- add capability to create dataset for an MPO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote connection parameters\n",
    "If data is stored remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: add putty connection too. \n",
    "\n",
    "#read SSH connection parameters\n",
    "with open('ssh_settings.json') as settings_file:    \n",
    "    settings = json.load(settings_file)\n",
    "\n",
    "hostname = settings['hostname']\n",
    "username = settings['username']\n",
    "password = settings['password']\n",
    "local_key_dir = settings['local_key_dir']\n",
    "\n",
    "census_dir = 'synthetic_population/'\n",
    "\"\"\"Remote directory with census data\"\"\"\n",
    "\n",
    "results_dir = 'craigslist_census/'\n",
    "\"\"\"Remote directory for results\"\"\"\n",
    "\n",
    "# estbalish SSH connection\n",
    "ssh = paramiko.SSHClient() \n",
    "ssh.load_host_keys(local_key_dir)\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "ssh.connect(hostname,username=username, password=password)\n",
    "sftp = ssh.open_sftp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_listings_file(fname):\n",
    "    \"\"\"Read csv file via SFTP and return as dataframe.\"\"\"\n",
    "    with sftp.open(listings_dir+fname) as f:\n",
    "        df = pd.read_csv(f, delimiter=',', dtype={'date':str,'fips_block':str,'state':str,'mpo_id':str})\n",
    "        # TODO: parse dates. \n",
    "    return df\n",
    "\n",
    "def log_var(x):\n",
    "    \"\"\"Return log of x, but NaN if zero.\"\"\"\n",
    "    if x==0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return np.log(x)\n",
    "    \n",
    "def create_census_vars(df):\n",
    "    \"\"\"Make meaningful variables and return the dataframe.\"\"\"\n",
    "    df['pct_white'] = df['race_of_head_1']/df['hhs_tot']\n",
    "    df['pct_black'] = df['race_of_head_2']/df['hhs_tot']\n",
    "    df['pct_amer_native'] = df['race_of_head_3']/df['hhs_tot']\n",
    "    df['pct_alaska_native'] = df['race_of_head_4']/df['hhs_tot']\n",
    "    df['pct_any_native'] = df['race_of_head_5']/df['hhs_tot']\n",
    "    df['pct_asian'] = df['race_of_head_6']/df['hhs_tot']\n",
    "    df['pct_pacific'] = df['race_of_head_7']/df['hhs_tot']\n",
    "    df['pct_other_race'] = df['race_of_head_8']/df['hhs_tot']\n",
    "    df['pct_mixed_race'] = df['race_of_head_9']/df['hhs_tot']\n",
    "    df['pct_mover'] = df['recent_mover_1']/df['hhs_tot']\n",
    "    df['pct_owner'] = df['tenure_1']/df['hhs_tot']\n",
    "    df['avg_hh_size'] = df['persons_tot']/df['hhs_tot']\n",
    "    df['cars_per_hh'] = df['cars_tot']/df['hhs_tot']\n",
    "    \n",
    "    df['ln_rent'] = df['rent'].apply(log_var)\n",
    "    df['ln_income'] = df.income_med.apply(log_var)\n",
    "    return df\n",
    "\n",
    "def filter_outliers(df, rent_range=(100,10000),sqft_range=(10,5000)):\n",
    "    \"\"\"Drop outliers from listings dataframe. For now, only need to filter out rent and sq ft. \n",
    "    Args: \n",
    "        df: Dataframe with listings. Cols names include ['rent','sqft']\n",
    "        rent_range (tuple): min and max rent\n",
    "        sqft_range (tuple): min and max sqft\n",
    "    Returns: \n",
    "        DataFrame: listings data without outliers. \n",
    "    \"\"\"\n",
    "    n0=len(df)\n",
    "    df=df[(df.rent>=rent_range[0])&(df.rent<rent_range[1])]\n",
    "    n1=len(df)\n",
    "    print('Dropped {} outside rent range ${}-${}'.format(n0-n1,rent_range[0],rent_range[1]))\n",
    "    df=df[(df.sqft>=sqft_range[0])&(df.sqft<sqft_range[1])]\n",
    "    n2=len(df)\n",
    "    print('Dropped {} outside sqft range {}-{} sqft. {} rows remaining'.format(n1-n2,sqft_range[0],sqft_range[1],len(df)))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get list of files and load. \n",
    "\n",
    "\n",
    "# for remotely stored data by state (just do one state for now)\n",
    "state='CA'\n",
    "infile='cl_census_{}.csv'.format(state)\n",
    "#data = read_listings_file(infile)  # uncomment to get remote data. \n",
    "\n",
    "# for local data: \n",
    "data_dir = '../data/'\n",
    "data_file = 'sfbay_listings_03032017.csv'\n",
    "\n",
    "data = pd.read_csv(data_dir+data_file,parse_dates=[1],dtype={'listing_id':str, 'rent':float, 'bedrooms':float, 'bathrooms':float, 'sqft':float,\n",
    "       'rent_sqft':float, 'fips_block':str, 'state':str, 'region':str, 'mpo_id':str, 'lng':float, 'lat':float,\n",
    "       'cars_tot':float, 'children_tot':float, 'persons_tot':float, 'workers_tot':float,\n",
    "       'age_of_head_med':float, 'income_med':float, 'hhs_tot':float, 'race_of_head_1':float,\n",
    "       'race_of_head_2':float, 'race_of_head_3':float, 'race_of_head_4':float, 'race_of_head_5':float,\n",
    "       'race_of_head_6':float, 'race_of_head_7':float, 'race_of_head_8':float, 'race_of_head_9':float,\n",
    "       'recent_mover_0':float, 'recent_mover_1':float, 'tenure_1':float, 'tenure_2':float})\n",
    "\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for census vars, NA really means 0...\n",
    "census_cols = ['cars_tot', 'children_tot','persons_tot', 'workers_tot', 'age_of_head_med', 'income_med','hhs_tot', 'race_of_head_1', 'race_of_head_2', 'race_of_head_3','race_of_head_4', 'race_of_head_5', 'race_of_head_6', 'race_of_head_7','race_of_head_8', 'race_of_head_9', 'recent_mover_0', 'recent_mover_1','tenure_1', 'tenure_2']\n",
    "for col in census_cols:\n",
    "    data[col] = data[col].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create variables\n",
    "**variable codes**\n",
    "\n",
    "Race codes (from PUMS)\n",
    "1 .White alone\n",
    " 2 .Black or African American alone\n",
    " 3 .American Indian alone\n",
    " 4 .Alaska Native alone\n",
    " 5 .American Indian and Alaska Native tribes specified; or American\n",
    " .Indian or Alaska native, not specified and no other races\n",
    " 6 .Asian alone\n",
    " 7 .Native Hawaiian and Other Pacific Islander alone\n",
    " 8 .Some other race alone\n",
    " 9 .Two or more major race groups\n",
    " \n",
    " \n",
    " \n",
    " tenure_1 = owner (based on my guess; didn't match the PUMS codes)\n",
    " \n",
    " mover_1 = moved past year (based on my guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create useful variables \n",
    "data = create_census_vars(data)\n",
    "\n",
    "# define some feature to include in the model. \n",
    "features_to_examine = ['rent','ln_rent', 'bedrooms','bathrooms','sqft','pct_white', 'pct_black','pct_asian','pct_mover','pct_owner','income_med','age_of_head_med','avg_hh_size','cars_per_hh']\n",
    "data[features_to_examine].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I've already identified these ranges as good at exluding outliers\n",
    "rent_range=(100,10000)\n",
    "sqft_range=(10,5000)\n",
    "data = filter_outliers(data, rent_range=rent_range, sqft_range=sqft_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this to explore outliers yourself. \n",
    "g=sns.distplot(data['rent'],  kde=False)\n",
    "g.set_xlim(0,10000)\n",
    "\n",
    "g=sns.distplot(data['sqft'], kde=False)\n",
    "g.set_xlim(0,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine NA's\n",
    "print('Total rows:',len(data))\n",
    "print('Rows with any NA:',len(data[pd.isnull(data).any(axis=1)]))\n",
    "print('Rows with bathroom NA:',len(data[pd.isnull(data.bathrooms)]))\n",
    "print('% rows missing bathroom col:',len(data[pd.isnull(data.bathrooms)])/len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uh oh, 74% are missing bathrooms feature. Might have to omit that one. Only 0.02% of rows have other missing values, so that should be ok. \n",
    "\n",
    "Bathrooms were added in Dec sometime. If bathrooms aren't in the listing, the listing is thrown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to drop NA's\n",
    "#data = data.dropna()\n",
    "#print('Dropped {} rows with NAs'.format(n0-len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at distributions\n",
    "\n",
    "Since rent has a more or less logarithmic distribution, use ln_rent instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p=sns.distplot(data.rent, kde=False)\n",
    "p.set_title('rent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p=sns.distplot(data.ln_rent, kde=False)\n",
    "p.set_title('ln rent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_rows = math.ceil(len(features_to_examine)/2)\n",
    "\n",
    "f, axes = plt.subplots(plot_rows,2, figsize=(8,15))\n",
    "sns.despine(left=True)\n",
    "\n",
    "for i,col in enumerate(features_to_examine):\n",
    "    row_position = math.floor(i/2)\n",
    "    col_position = i%2\n",
    "    data_notnull = data[pd.notnull(data[col])]  # exclude NA values from plot\n",
    "    sns.distplot(data_notnull[col], ax=axes[row_position, col_position],kde=False)\n",
    "    axes[row_position, col_position].set_title('{}'.format(col)) \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_notnull = data[pd.notnull(data['ln_income'])]\n",
    "p=sns.distplot(data_notnull['ln_income'],kde=False)\n",
    "p.set_title('ln med income')\n",
    "# ln med income is not more normal.. use med income instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## look at correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correlation heatmap\n",
    "corrmat=data[features_to_examine].corr()\n",
    "corrmat.head()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlations appear as expected, except for cars_per_hh. Maybe this is because cars_per_hh is reflecting the size of the household more than income. Might want to try cars per adult instead.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Try a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data.columns)\n",
    "#'pct_amer_native','pct_alaska_native',\n",
    "x_cols = ['bedrooms','bathrooms', 'sqft','age_of_head_med', 'income_med','pct_white', 'pct_black', 'pct_any_native', 'pct_asian', 'pct_pacific',\n",
    "       'pct_other_race', 'pct_mixed_race', 'pct_mover', 'pct_owner', 'avg_hh_size', 'cars_per_hh']\n",
    "y_col = 'ln_rent'\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create training and testing datasets. \n",
    "\n",
    "# exclude missing values\n",
    "data_notnull= data[(pd.notnull(data[x_cols])).all(axis=1)]\n",
    "data_notnull= data_notnull[(pd.notnull(data_notnull[y_col]))]\n",
    "print('using {} rows of {} total'.format(len(data_notnull),len(data)))\n",
    "\n",
    "# this creates a test set that is 30% of total obs.  \n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(data_notnull[x_cols],data_notnull[y_col], test_size = .3, random_state = 201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Intercept\n",
    "print('Intercept:', regr.intercept_)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:')\n",
    "pd.Series(regr.coef_, index=x_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See mean square error, using test data\n",
    "print(\"Mean squared error: %.2f\" % np.mean((regr.predict(X_test) - y_test) ** 2))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % regr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot predicted values vs. observed\n",
    "plt.scatter(regr.predict(X_train),y_train, color='blue',s=1, alpha=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot residuals vs predicted values\n",
    "plt.scatter(regr.predict(X_train), regr.predict(X_train)- y_train, color='blue',s=1, alpha=.5)\n",
    "plt.scatter(regr.predict(X_test), regr.predict(X_test)- y_test, color='green',s=1, alpha=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals look pretty normally distributed.\n",
    "\n",
    "I wonder if inclusion of all these race variables is leading to overfitting. If so, we'd have small error on training set and large error on test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Training set. Mean squared error: %.5f\" % np.mean((regr.predict(X_train) - y_train) ** 2), '| Variance score: %.5f' % regr.score(X_train, y_train))\n",
    "print(\"Test set. Mean squared error: %.5f\" % np.mean((regr.predict(X_test) - y_test) ** 2), '| Variance score: %.5f' % regr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Ridge Regression (linear regression with regularization )\n",
    "Since the training error and test error are about the same, and since we're using few features, overfitting probably isn't a problem. If it were a problem, we would want to try a regression with regularization. \n",
    "Let's try it just for the sake of demonstration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try a range of different regularization terms.\n",
    "for a in [10,1,0.1,.01,.001,.00001]:\n",
    "    ridgereg = Ridge(alpha=a)\n",
    "    ridgereg.fit(X_train, y_train)\n",
    "    \n",
    "    print('\\n alpha:',a)\n",
    "    print(\"Mean squared error: %.5f\" % np.mean((ridgereg.predict(X_test) - y_test) ** 2),'| Variance score: %.5f' % ridgereg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Intercept\n",
    "print('Intercept:', ridgereg.intercept_)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:')\n",
    "pd.Series(ridgereg.coef_, index=x_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, Ridge regression doesn't help much. \n",
    "The best way to improve the model at this point is probably to add more features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ways to improve the model:\n",
    "- add features. E.g., dummy vars for MPO, population density. Maybe date?\n",
    "- at least filter for date\n",
    "- other census variables (e.g., ACS building chars)\n",
    "- try robust regression\n",
    "- try RF, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
